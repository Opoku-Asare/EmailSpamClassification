{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing and Text Mining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Email Spam Classification Demo\n",
    "\n",
    "\n",
    "\n",
    "## By: Asare & Ashikur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Dependencies\n",
    "# 1. numpy\n",
    "# 2. scipy\n",
    "# 2. scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate,cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer,scale\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import grammar_check\n",
    "\n",
    "\n",
    "\n",
    "class EmailClassifier():\n",
    "    \"\"\"\n",
    "     Classifies emails into spam or ham based on several features \n",
    "     The Classifier uses the Support Vector Machine, Random Forest, Naive Bayesian, K Nearest Neighbour\n",
    "     The classifier using cross validation in training and evaluation, and also implements a majority votinig rule\n",
    "     to classify emails into spam or ham\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # run this method onces, and then load the saved data and use subsequently\n",
    "    # method saves dict_enron.npy, all_email_corpus\n",
    "   \n",
    "    def make_Dictionary(root_dir):\n",
    "        all_email_corpus = {'text': [], 'class': []}\n",
    "\n",
    "        emails_dirs = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        all_words = []\n",
    "        for emails_dir in emails_dirs:\n",
    "            dirs = [os.path.join(emails_dir, f) for f in os.listdir(emails_dir)]\n",
    "            for d in dirs:\n",
    "                emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "                for mail in emails:\n",
    "                    with open(mail) as m:\n",
    "                        email_words = []\n",
    "                        for line in m:\n",
    "                            words = nltk.word_tokenize(line)  # line.split()\n",
    "                            all_words += words\n",
    "                            email_words += words\n",
    "                        emailClass = 'ham'\n",
    "                        print mail.split(\".\")[-2]\n",
    "                        if mail.split(\".\")[-2] == 'spam':\n",
    "                            emailClass = 'spam'\n",
    "                        all_email_corpus['text'].append(' '.join(email_words))\n",
    "                        all_email_corpus['class'].append(\n",
    "                            emailClass)  # 1 is spam , 0 is ham\n",
    "\n",
    "        dictionary = Counter(all_words)\n",
    "        list_to_remove = dictionary.keys()\n",
    "\n",
    "        for item in list_to_remove:\n",
    "            if item.isalpha() == False:\n",
    "                del dictionary[item]\n",
    "            elif len(item) == 1:\n",
    "                del dictionary[item]\n",
    "        dictionary = dictionary.most_common(3000)\n",
    "        vocabulary = sorted([key for (key, value) in dictionary])\n",
    "        np.save('vocabulary.npy', vocabulary)\n",
    "        np.save('all_email_corpus.npy', all_email_corpus)\n",
    "\n",
    "        return vocabulary, all_email_corpus\n",
    "\n",
    "   \n",
    "    def classify_emails(self,SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables):\n",
    "\n",
    "            classifier_Labels=[\"SVM\",\"Naive Bayesian\",\"Random Forest\",\"K Nearest Neighbour(K=5)\",\"Majority Vote\"]\n",
    "            majorityVoteClassifier=VotingClassifier(estimators=[(\"SVM\",SVM_pipeline),(\"Naive Bayesian\",NB_pipeline),(\"Random Forest\",RF_pipeline),(\"K-Nearest Neigbour\",KNN_pipeline)],voting='hard')\n",
    "            for clf, label in zip([SVM_pipeline, NB_pipeline, RF_pipeline,KNN_pipeline, majorityVoteClassifier], classifier_Labels):\n",
    "                scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "                crossValidationResults = cross_validate(clf, documents, binarisedLables, cv=5, scoring=scoring,return_train_score=False)\n",
    "                accuracy=crossValidationResults['test_accuracy']\n",
    "                precision=crossValidationResults['test_precision']\n",
    "                recall=crossValidationResults['test_recall']\n",
    "                f1=crossValidationResults['test_f1']\n",
    "                metrics = {}\n",
    "                metrics[\"Classifier\"] = label\n",
    "                metrics[\"Recall\"] = \"%0.2f (+/- %0.2f)\" % (recall.mean(),recall.std())\n",
    "                metrics[\"Precision\"] = \"%0.2f (+/- %0.2f)\" % (precision.mean(),precision.std())\n",
    "                metrics[\"F1 Score\"] = \"%0.2f (+/- %0.2f)\" % (f1.mean(),f1.std())\n",
    "                metrics[\"Accuracy\"] = \"%0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std())\n",
    "\n",
    "            \n",
    "\n",
    "                y_pred = cross_val_predict(clf, documents, binarisedLables)\n",
    "                allSpam=[index for index,value in enumerate(y_pred) if value==1]\n",
    "                allHam=[index for index,value in enumerate(y_pred) if value==0]\n",
    "                spamCount= len(allSpam)\n",
    "                HamCount= len(allHam)\n",
    "                metrics[\"Spam Count\"] = spamCount\n",
    "                metrics[\"Ham Count\"] = HamCount\n",
    "                print metrics\n",
    "\n",
    "                if label==\"Majority Vote\":\n",
    "                    sampledSpam=random.sample(allSpam,5)\n",
    "                    print \"\\n\\n5 randomly sample spam examples\\n\"\n",
    "                    for entry in sampledSpam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                        print str(entry)+\"\\n\"\n",
    "                        emailType=binarisedLables[entry]\n",
    "                        if emailType==1:\n",
    "                            print (\"It is actually a spam\")\n",
    "                        if emailType==0:\n",
    "                            print (\"It is actually a ham\")\n",
    "                            \n",
    "                    sampledHam=random.sample(allHam,5)\n",
    "                    print \"\\n\\n5 randomly sample ham examples\\n\"\n",
    "                    print binarisedLables\n",
    "                    for entry in sampledHam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                        print str(entry)+\"\\n\"\n",
    "                        emailType=binarisedLables[entry]\n",
    "                        if emailType==1:\n",
    "                            print (\"It is actually a spam\")\n",
    "                        if emailType==0:\n",
    "                            print (\"It is actually a ham\")\n",
    "                           \n",
    "                        \n",
    "                        \n",
    "                scoring=[]\n",
    "                accuracy=[]\n",
    "                recall=[]\n",
    "                precision=[]\n",
    "                f1=[]\n",
    "                metrics={}\n",
    "                crossValidationResults={}\n",
    "                allHam=[]\n",
    "                allSpam=[]\n",
    "                \n",
    "    \n",
    "    def evaluate_prediction(self,labels_test, predictions):\n",
    "        evaluationTable = []\n",
    "        for key, value in predictions.iteritems():\n",
    "            confusion_matrix\n",
    "            evaluation = {}\n",
    "            evaluation[\"Classifier\"] = key\n",
    "\n",
    "            evaluation[\"Recall\"] = recall_score(labels_test, value)\n",
    "            evaluation[\"Precision\"] = precision_score(labels_test, value)\n",
    "            evaluation[\"F1 Score\"] = f1_score(labels_test, value)\n",
    "            evaluation[\"Average Precision score\"] = average_precision_score(\n",
    "                labels_test, value)\n",
    "            evaluation[\"tn\"],evaluation[\"fp\"],evaluation[\"fn\"],evaluation[\"tp\"] = confusion_matrix(labels_test, value).ravel()\n",
    "            evaluationTable.append(evaluation)\n",
    "        return evaluationTable\n",
    "\n",
    "    def using_TF(self):\n",
    "        \"\"\"Uses term frequency feature for classification\"\"\"\n",
    "        print \"With TF\"\n",
    "        word2vectTransformer=CountVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_TFIDF(self):\n",
    "        \"\"\"Uses term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "        print \"With TF.IDF\"\n",
    "\n",
    "        java_path = \"C:/Program Files/Java/jdk-9.0.1/bin/java.exe\"\n",
    "        os.environ['JAVAHOME'] = java_path\n",
    "        nltk.internals.config_java(java_path)\n",
    "\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_F1_F2_F3_F4_F5(self):\n",
    "        \"\"\"Uses the following features for classification,\n",
    "            - Counts of Urls\n",
    "            - Count of Language Mistakes\n",
    "            - Count of words\n",
    "            - Count of named entities\n",
    "        \"\"\"\n",
    "\n",
    "        print \"With feature set F1.F2.F3.F4\"\n",
    "\n",
    "        #Note LanguateMistakesVectorizer requires a running grammar-check server. check readme.md\n",
    "        featureSet=FeatureUnion([\n",
    "            ('Count of URLs',URLCountVectorizer()),\n",
    "            ('Count of Language Mistakes',LanguageMistakesVectorizer()),\n",
    "            ('Count of words',WordCountVectorizer()),\n",
    "            ('Count of Named Entities',NameEntityCountVectorizer())\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Components of term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "\n",
    "        print \"With PCA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses linear discriminant analyses of term frequency x Inverse document \n",
    "        frequency feature for classification\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Using LDA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_and_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Componets and Linear discriminant analyses of\n",
    "         term frequency x Inverse document frequency feature for classification\n",
    "         \"\"\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        print \"Using PCA (TF.IDF), LDA (TF.IDF)\"\n",
    "        \n",
    "        featureSet=FeatureUnion([\n",
    "            ('PCA (TF.IDF)',PCA(n_components=10)),\n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10))\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_suggested_features(self):\n",
    "        \"\"\"Uses targetted word list occurrence counts feature for classification\"\"\"\n",
    "\n",
    "        print \"Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\"\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "\n",
    "\n",
    "class WordCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts word counts in the document\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_words_doc(self, doc):\n",
    "        \"\"\"Returns the count of words in a document\"\"\"\n",
    "        all_words = nltk.word_tokenize(doc)\n",
    "        all_words_iter = all_words\n",
    "        for item in all_words_iter:\n",
    "            if item.strip().isalpha() == False:\n",
    "                all_words.remove(item)\n",
    "\n",
    "        return len(all_words)\n",
    "\n",
    "    def get_all_word_counts(self, docs):\n",
    "         \"\"\"Encodes document to number of words\"\"\"\n",
    "         return [self.count_words_doc(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList = self.get_all_word_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class NameEntityCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a document and extracts count of named entities.\n",
    "        This class uses the NLTK parts of speach tagger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_named_entities(self, doc):\n",
    "        \"\"\"Returns a count of named entities in te a document\"\"\"\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        #serFile=os.path.join(dir_path,'english.all.3class.distsim.crf.ser')\n",
    "        #jarFile=os.path.join(dir_path,'stanford-ner.jar')\n",
    "        #st=StanfordNERTagger(serFile,jarFile,encoding='utf-8')\n",
    "        #tokens_NER=tokens\n",
    "        #for tk in tokens:\n",
    "        #    if len(tk)==1:\n",
    "        #        tokens_NER.remove(tk)\n",
    "        #word_tag=st.tag(tokens_NER)\n",
    "    \n",
    "        named_entities_tag =[]\n",
    "        #for tag in word_tag:\n",
    "        #    if tag[1]!='O':\n",
    "        #        named_entities_tag.append(tag)\n",
    "    \n",
    "        \n",
    "\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        \n",
    "        ##NN\tNoun, singular or mass\n",
    "        ##NNS\tNoun, plural\n",
    "        ##NNP\tProper noun, singular\n",
    "        ##NNPS\tProper noun, plural\n",
    "        #namedEntityTags_set1 = [ \"NN\",\"NNS\" ]#\"NNPS\"\"NN\",\"NNS\"\"NNP\",\"NNPS\"\n",
    "        namedEntityTags_set2 = [ \"NNP\" ]#\"NNPS\"\"NNPS\"\n",
    "        named_entities = []\n",
    "        named_entities_tag =[]\n",
    "       \n",
    "        for word, tag in pos:\n",
    "            if tag in namedEntityTags_set2:\n",
    "                if word.isalpha() and len(word)>1:\n",
    "                   named_entities_tag.append(tag)\n",
    "                   named_entities.append(word)\n",
    "       # print named_entities\n",
    "\n",
    "    \n",
    "        return len(named_entities_tag)\n",
    "\n",
    "    def get_all_named_entities(self, docs):\n",
    "        \"\"\"Encodes document to number of named entities\"\"\"\n",
    "        return [self.count_named_entities(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_named_entities(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class URLCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of URLs,links in document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_url_links(self, s):\n",
    "        \"\"\"Returns number of emails found in text\"\"\"\n",
    "        express1=\"((?:(http s?|s?ftp):\\/\\/)?(?: www \\.)?((?:(?:[A-Z0-9][A-Z0-9-]{0,61}[A-Z0-9]\\.)+)([A-Z]{2,6})|(?:\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}))(?::(\\d{1,5}))?(?:(\\/\\S+)*))\"\n",
    "        express2=\"http [s]?:// (?: www \\.)? (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        express3=\"http|www|goto\"\n",
    "\n",
    "        re1='(http|https|goto)'\t# Word 1\n",
    "        re2='(\\\\s+)'\t# White Space 1\n",
    "        re3='(.)'\t# Any Single Character 1\n",
    "        re4='(\\\\s+)'\t# White Space 2\n",
    "        re5='(.)'\t# Any Single Character 2\n",
    "        re6='(\\\\s+)'\t# White Space 3\n",
    "        re7='(\\\\/)'\t# Any Single Character 3\n",
    "        re8='.*?'\t# Non-greedy match on filler\n",
    "        re9='((www)*)'\t# Word 2\n",
    "        re10='(\\\\s+)'\t# White Space 4\n",
    "        regex = re.compile(re1+re2+re3+re4+re5+re6+re7+re8+re9+re10,re.IGNORECASE|re.DOTALL)\n",
    "        \n",
    "        emails=re.findall(regex, s)\n",
    "\n",
    "        return len(emails)\n",
    "\n",
    "    def get_all_url_counts(self, docs):\n",
    "        \"\"\"Encodes document to number of URL, links\"\"\"\n",
    "        \n",
    "        return [self.count_url_links(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_url_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class TargetedWordCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts the count of targeted words in the each document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_targeted_words(self,doc):\n",
    "        target_words=[\"£\",\"$\",\"%\",\"!\" \"viagra\", \"penis\", \"billion\", \"billionaire\", \"lottery\", \"prize\", \"charity\" , \"USA\", \"Nigeria\"]\n",
    "        target_hit_count= len([ word for word in target_words if word in doc ])\n",
    "        return target_hit_count\n",
    "        \n",
    "    def get_all_targeted_words_count(self,docs):\n",
    "        return [self.count_targeted_words(d) for d in docs]\n",
    "        \n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_targeted_words_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class LanguageMistakesVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of English language Mistakes\"\"\"\n",
    "    _defaultLanguage='en-GB'\n",
    "    def __init__(self,language=None):\n",
    "        if language:\n",
    "           self._defaultLanguage=language\n",
    "        pass\n",
    "    def count_language_mistakes(self, doc):\n",
    "        \"\"\"Returnes the count of English mistakes in the text \"\"\"\n",
    "        tool=grammar_check.LanguageTool(self._defaultLanguage)\n",
    "        encodedText=doc.decode(\"utf-8\",errors='replace')\n",
    "        try:\n",
    "            mistakes=tool.check(encodedText)\n",
    "            \n",
    "        except Exception  as e:\n",
    "            mistakes=[]\n",
    "        \n",
    "        \n",
    "        return len(mistakes)\n",
    "    def get_all_mistake_count(self,docs):\n",
    "         \"\"\"Encodes document to number of Language mistakes\"\"\"\n",
    "         return [self.count_language_mistakes(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_mistake_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \"Takes a sparse array and converts it to dense array\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "    \n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "class PCAScaleTranformer(TransformerMixin):\n",
    "    \"Takes PCA Matrix and coverts to entries of absolute values\"\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "       \n",
    "        return np.absolute(X)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read saved corpus and Initialise Email Classifier \n",
    "\n",
    "### Limited dataset 200 emails (100 spam , 100 ham)\n",
    "#### This dataset has been loaded and said to a numpy array using make_Dictionary in EmailClassifier class above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_email_corpus=np.load(\"./old/all_email_corpus.npy\").item()\n",
    "vocabularyList=np.load(\"./old/vocabulary.npy\").tolist()\n",
    "documents=all_email_corpus['text']\n",
    "labels=all_email_corpus['class']\n",
    "binarizer=LabelBinarizer()\n",
    "binarisedLables=binarizer.fit_transform(labels).ravel()\n",
    "\n",
    "\n",
    "emailclassifier= EmailClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using TF.IDF feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TF.IDF\n",
      "{'Recall': '0.99 (+/- 0.02)', 'Precision': '0.97 (+/- 0.04)', 'Spam Count': 107, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 93, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.91 (+/- 0.06)', 'Precision': '1.00 (+/- 0.00)', 'Spam Count': 91, 'F1 Score': '0.95 (+/- 0.03)', 'Classifier': 'Naive Bayesian', 'Ham Count': 109, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.91 (+/- 0.06)', 'Precision': '0.91 (+/- 0.05)', 'Spam Count': 112, 'F1 Score': '0.91 (+/- 0.04)', 'Classifier': 'Random Forest', 'Ham Count': 88, 'Accuracy': '0.91 (+/- 0.04)'}\n",
      "{'Recall': '0.97 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 93, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 107, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.95 (+/- 0.03)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 98, 'F1 Score': '0.97 (+/- 0.03)', 'Classifier': 'Majority Vote', 'Ham Count': 102, 'Accuracy': '0.97 (+/- 0.02)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: re [ 13 ] driving at ? in 1876 dogs and cats that ' s a call for you glrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know creai femaie ejacuiation ! clever 9 thef \\ / \\ / ettest pussles ! you ' d better not . . to | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s ! engine 30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l ! in 1893 good night ! e | \\ | ter ! don ' t look very fit and i can in 1968 date of birth in 1927 eye one } > loook at yes , it ' s me . in 1800 in 1867 in 1842 city name or\n",
      "\n",
      "Subject: from mrs . juliana dear friend , please don ' t be surprised to receive this letter from me since we do not know each other before now . i am mrs . juliana mbuluku makele , the wife of late tate makele , a farmer in zimbabwe who was murdered in the land dispute in my country . i got your contact through network online hence decided to write you . before the death of my husband , he made a deposit with a security firm . the consignment was declared to contain gem stones shipped on diplomatic cover to avoid customs detecting the actual contents . the actual content is the sum of us 25 . 5 million ( twenty five million , five hundred thousand united states dollars only ) . this was done when he noticed the looming danger in zimbabwe on land policies . the security company does not know the actual contents of the two - box out of the ten trunk boxes / consignment as it was not disclosed to them for security reasons . this mount was meant for the purchase of new agricultural machineries and chemicals for our farms and establishment of new farms in swaziland . this land problem came when zimbabwean president mr . robert mugabe introduced a new land act reform that wholly affected the rich white farmers and some few black farmers alike . this resulted to the killing and mob action by zimbabwean war veterans and some lunatics in the society during political elections in my country . in fact a lot of people were killed because of this land reform act for which my husband was one of the victims . it is against this background that , all my family members fled zimbabwe for fear of our lives . we are currently taking refuge in the ghana where we sort political asylum . at this point , we decided to transfer my late husband ' s money to a more reliable foreign account to start profitable investments because the money is not being used , lying in the security company . i must let you know that this transaction is free from risk because i have all the necessary documents . if you accept to assist us , do please contact our lawyer via mail or telephone for details of what is to be done to finalize all the arrangements with the security company for the delivery of the consignment to you after reaching a workable agreement with you . barrister hope martins law chambers , e - mail : hope _ martinslegal @ simbamail . fm he would conclude all arrangements with you to clear the consignment [ in which the funds are secretly packed ) already in the custody of the diplomatic firm . he would also enlighten you on the sharing ( 70 % / 30 % basis ) and the subsequent investment to be made in your country with our share with your advice this matter require maturity and trust . regards , mrs . juliana mbuluku for : ( the family ) . please you can read about problems in zimbabwe from the links below :\n",
      "\n",
      "Subject: paliourg udtih 7 wcwknoanopkt good morning paliourg ! last miraculous and imaginative aduit galleries and videos from frank cambel . at the moment theme is pleasant and most excellent cars ! maria is washing her bmw ! sara trant choosing volkswagen or honda in car shop ! test this homepage for all wonderful photographs and videos : p . s . admission is 100 % free , distressing aduit snapshots and videos only ! best regards , supporter . dear paliourg send any gratis mail here mailto : eilao 99 @ online . com . ua to discontinue if this is hard mess .\n",
      "\n",
      "Subject: re : aqemkfg , ordered it copied free cable % rnd _ syb tv comparison ak brain consist iniquitous twit trigram trajectory buteo buttock steve farad algerian argonaut yen edison loquacious headquarter shell bouncy temporal birdseed caliper deity combine gallberry hub pretoria apocalyptic audrey pittsburgh arcsin aba beatific sousa benight evasion devout mao coercion icy endgame channel repository binocular ahead horseplay parboil confidential crestfallen sought nan confect obtrude seashore decker radiotherapy\n",
      "\n",
      "Subject: urgent news alert ! ( otcbb : gspm ) gold is hot ! stock profile company name golden spirit minerals ltd stock symbol otcbb : gspm current price $ 0 . 044 shares outstanding 40 . 7 million approx . float 29 . 2 million gspm highlights corporate profile press release important disclaimer and information : verify all claims and do your own due diligence . stocks to play ( s 2 p ) profiles are not a solicitation or recommendation to buy , sell or hold securities . s 2 p is not offering securities for sale . all statements and expressions are the sole opinion of s 2 p and are subject to change without notice . the companies that are discussed in this opinion have not approved the statements made in this opinion . this release may contain statements that constitute forward - looking statements within the meaning of section 27 a of the securities act of 1933 , as amended , and section 21 e the securities exchange act of 1934 , as amended . the words \" may \" \" would , \" \" will , \" \" expect , \" \" estimate , \" \" anticipate , \" \" believe , \" \" intend , \" and similar expressions and variations thereof are intended to identify forward - looking statements . investors are cautioned that any such forward - looking statements are not guarantees of future performance and involve risks and uncertainties , many of which are beyond the company ' s ability to control , and that actual results may differ materially from those projected in the forward - looking statements as a result of various factors . this profile is not without bias , and is a paid advertisement . s 2 p is not liable for any investment decisions by its readers or subscribers . s 2 p is not a registered broker dealer or investment advisor . it is strongly recommended that any purchase or sale decision be discussed with a financial adviser , or a broker - dealer , or a member of any financial regulatory bodies . the information contained in s 2 p profiles is provided as an information only service . the accuracy or completeness of the information is not guaranteed and is only as reliable as the sources from which it was obtained . investing in micro cap stocks is extremely risky and , investors are cautioned that they may lose all or a portion of their investment if they make a purchase in gspm . s 2 p has been compensated two million free trading shares of gspm by a third party non - affiliate ( charlton investments ltd ) . the reader should verify all claims and do their own due diligence before investing in any securities mentioned . investing in securities is speculative and carries a high degree of risk . we encourage our readers to invest carefully and read the investor information available at the web sites of the securities and exchange commission ( \" sec \" ) at http : / / www . sec . gov and / or the national association of securities dealers ( \" nasd \" ) at http : / / www . nasd . com . we also strongly recommend that you read the sec advisory to investors concerning internet stock fraud , which can be found at http : / / www . sec . gov / consumer / cyberfr . htm . readers can review all public filings by companies at the sec ' s edgar page . the nasd has published information on how to invest carefully at its web site . s 2 p intends to sell all or a portion of the gspm stock at or about the time of publication of this report . subsequently s 2 p may buy or sell shares of gspm stock in the open market . since s 2 p has been compensated there is an inherent conflict of interest . this message was sent to address paliourg @ iit . demokritos . gr to stop receiving emails from this recurring list , send a blank email to unsub - xceescerwmikb @ stocksnut . comorsend a mail with your email address to now technologies , 2234 north federal highway suite 481 , boca raton , florida 33431\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Subject: jan noms - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 12 / 28 / 99 10 : 51 am - - - - - - - - - - - - - - - - - - - - - - - - - - - troy _ a _ benoit @ reliantenergy . com on 12 / 28 / 99 10 : 47 : 23 am to : ami chokshi / corp / enron @ enron cc : subject : jan noms ( see attached file : egmnom - jan . xls ) - egmnom - jan . xls\n",
      "\n",
      "56\n",
      "\n",
      "It is actually a ham\n",
      "Subject: january - meter 2186 clear lake city gate i have flow without a nom each day at meter 2186 lst = 1375 2 nd = 28 3 rd = 2532 4 th = 5952 i assume that this is entex meter . is this volume captured at meter 2000 ? i will need a nom to support this flow . please advise . thank you .\n",
      "\n",
      "94\n",
      "\n",
      "It is actually a ham\n",
      "Subject: koch three rivers , # 6722 the outage is scheduled for 1 / 4 / 00 to 1 / 6 / 00 . it will not effect 1 / 1 / 00 throught 1 / 3 / 00 . there is no need for duke to hold gas back until 1 / 4 / 00 . gary - - - - - - - - - - - - - - - - - - - - - - forwarded by gary a hanks / hou / ect on 12 / 30 / 99 10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - - from : charlotte hawkins 12 / 30 / 99 09 : 07 am to : pat clynes / corp / enron @ enron , lauri a allen / hou / ect @ ect , daren j farmer / hou / ect @ ect , elsa villarreal / hou / ect @ ect , edward d gottlob / hou / ect @ ect cc : aimee lannou / hou / ect @ ect , gary a hanks / hou / ect @ ect subject : koch three rivers , # 6722 i spoke with larry zamit this morning regarding the outage january 1 through 6 . he agreed to hold the gas for us and batch it throughout the rest of january . at this point we have 3 , 007 nominated for 1 / 1 / 00 and that is what he has agreed to hold . questions ? ? give me a call . charlotte / aimee\n",
      "\n",
      "77\n",
      "\n",
      "It is actually a ham\n",
      "Subject: re : missing service rates for ena / hplc cash desk - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 01 / 04 / 2000 10 : 36 am - - - - - - - - - - - - - - - - - - - - - - - - - - - rhonda robinson @ ect 01 / 04 / 2000 10 : 23 am to : stacey neuweiler / hou / ect @ ect , ami chokshi / corp / enron @ enron cc : gregory steagall / hou / ect @ ect subject : re : missing service rates for ena / hplc cash desk listed below are expense contracts that need rates input into sitara . please let me know as soon as possible if you are responsible for the input of these rates . thanks ! - - - - - - - - - - - - - - - - - - - - - - forwarded by rhonda robinson / hou / ect on 01 / 04 / 2000 10 : 17 am - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : heidi withers 01 / 04 / 2000 09 : 18 am to : rhonda robinson / hou / ect @ ect cc : subject : re : missing service rates for ena / hplc cash desk okay ! how about the other expense rates ? if you need any help getting those input , let me know . otherwise we ' ll make sure things are clean for that 11 th workday . i hope you had a great new year ' s ! heidi rhonda robinson 01 / 04 / 2000 09 : 13 am to : heidi withers / hou / ect @ ect cc : subject : re : missing service rates for ena / hplc cash desk theresa in denver is looking into the missing rates issue for powder river gathering . thanks for all of your help . reminder : we will be beginning our close process on the 11 th workday . enron north america corp . from : heidi withers 12 / 31 / 99 10 : 07 am to : rhonda robinson / hou / ect @ ect , gregory steagall / hou / ect @ ect cc : charlotte hawkins / hou / ect @ ect , mary m smith / hou / ect @ ect subject : missing service rates for ena / hplc cash desk rhonda , i took a look at the missing service rates report and i don ' t have the rates to input into the system . the best i can do is input them if you provide them to me . i am sorry that i can ' t help more . . . call me if you need to at xl 836 . greg sent me the expense reconciliation reports that have come out were : powder river gathering - expense powder river gathering - revenue east texas gathering - expense king ranch gas processing - expense humble gas pipeline - expense channel industries - expense tgpl - expense black marlin - expense i ' m here till noon , and reachable after that at 713 - 385 - 7612 . i think that charlotte or someone in the logistics group can help with these rates .\n",
      "\n",
      "81\n",
      "\n",
      "It is actually a ham\n",
      "Subject: system information - january 5 th christmass s @ | e - w ! ndows xp home we have everything ! wlndows x , p professional 2 oo 2 . . . . . . . . . . . 50 adobe photoshop 7 . o . . . . . . . . . . . . . . . . . . . . . . . . 6 o microsoft office x . p pro 2 oo 2 . . . . . . . . . . . . . . 6 o corel draw graphics suite 11 . . . . . . . . . . . . . 60 get it quickly : http : / / demagnify . goforthesoft . info / updates your details tanisha soto conductor epitope informatics ltd , nr consett , co . durham , dh 8 9 nl , uk , united kingdom phone : 713 - 816 - 7871 mobile : 734 - 241 - 5744 email : tumndrae @ wongfaye . com this message is beng sent to confirm your account . please do not reply directly to this message this product is a 79 month trial software notes : the contents of this info is for understanding and should not be muddle paralysis shamrock snuffer dragging time : sun , 11 jan 2004 11 : 08 : 30 + 0200\n",
      "\n",
      "149\n",
      "\n",
      "It is actually a spam\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using PCA of TF.IDF (10 principal components) as a feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "emailclassifier.using_PCA_of_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using suggested feature set i.e targetted  word list feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emailclassifier.using_suggested_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
