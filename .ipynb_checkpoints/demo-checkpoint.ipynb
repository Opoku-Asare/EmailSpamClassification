{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing and Text Mining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Email Spam Classification Demo\n",
    "\n",
    "\n",
    "\n",
    "## By: Asare & Ashikur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Dependencies\n",
    "# 1. numpy\n",
    "# 2. scipy\n",
    "# 2. scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate,cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer,scale\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import grammar_check\n",
    "\n",
    "\n",
    "\n",
    "class EmailClassifier():\n",
    "    \"\"\"\n",
    "     Classifies emails into spam or ham based on several features \n",
    "     The Classifier uses the Support Vector Machine, Random Forest, Naive Bayesian, K Nearest Neighbour\n",
    "     The classifier using cross validation in training and evaluation, and also implements a majority votinig rule\n",
    "     to classify emails into spam or ham\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # run this method onces, and then load the saved data and use subsequently\n",
    "    # method saves dict_enron.npy, all_email_corpus\n",
    "   \n",
    "    def make_Dictionary(root_dir):\n",
    "        all_email_corpus = {'text': [], 'class': []}\n",
    "\n",
    "        emails_dirs = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        all_words = []\n",
    "        for emails_dir in emails_dirs:\n",
    "            dirs = [os.path.join(emails_dir, f) for f in os.listdir(emails_dir)]\n",
    "            for d in dirs:\n",
    "                emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "                for mail in emails:\n",
    "                    with open(mail) as m:\n",
    "                        email_words = []\n",
    "                        for line in m:\n",
    "                            words = nltk.word_tokenize(line)  # line.split()\n",
    "                            all_words += words\n",
    "                            email_words += words\n",
    "                        emailClass = 'ham'\n",
    "                        print mail.split(\".\")[-2]\n",
    "                        if mail.split(\".\")[-2] == 'spam':\n",
    "                            emailClass = 'spam'\n",
    "                        all_email_corpus['text'].append(' '.join(email_words))\n",
    "                        all_email_corpus['class'].append(\n",
    "                            emailClass)  # 1 is spam , 0 is ham\n",
    "\n",
    "        dictionary = Counter(all_words)\n",
    "        list_to_remove = dictionary.keys()\n",
    "\n",
    "        for item in list_to_remove:\n",
    "            if item.isalpha() == False:\n",
    "                del dictionary[item]\n",
    "            elif len(item) == 1:\n",
    "                del dictionary[item]\n",
    "        dictionary = dictionary.most_common(3000)\n",
    "        vocabulary = sorted([key for (key, value) in dictionary])\n",
    "        np.save('vocabulary.npy', vocabulary)\n",
    "        np.save('all_email_corpus.npy', all_email_corpus)\n",
    "\n",
    "        return vocabulary, all_email_corpus\n",
    "\n",
    "   \n",
    "    def classify_emails(self,SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables):\n",
    "\n",
    "            classifier_Labels=[\"SVM\",\"Naive Bayesian\",\"Random Forest\",\"K Nearest Neighbour(K=5)\",\"Majority Vote\"]\n",
    "            majorityVoteClassifier=VotingClassifier(estimators=[(\"SVM\",SVM_pipeline),(\"Naive Bayesian\",NB_pipeline),(\"Random Forest\",RF_pipeline),(\"K-Nearest Neigbour\",KNN_pipeline)],voting='hard')\n",
    "            for clf, label in zip([SVM_pipeline, NB_pipeline, RF_pipeline,KNN_pipeline, majorityVoteClassifier], classifier_Labels):\n",
    "                scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "                crossValidationResults = cross_validate(clf, documents, binarisedLables, cv=5, scoring=scoring,return_train_score=False)\n",
    "                accuracy=crossValidationResults['test_accuracy']\n",
    "                precision=crossValidationResults['test_precision']\n",
    "                recall=crossValidationResults['test_recall']\n",
    "                f1=crossValidationResults['test_f1']\n",
    "                metrics = {}\n",
    "                metrics[\"Classifier\"] = label\n",
    "                metrics[\"Recall\"] = \"%0.2f (+/- %0.2f)\" % (recall.mean(),recall.std())\n",
    "                metrics[\"Precision\"] = \"%0.2f (+/- %0.2f)\" % (precision.mean(),precision.std())\n",
    "                metrics[\"F1 Score\"] = \"%0.2f (+/- %0.2f)\" % (f1.mean(),f1.std())\n",
    "                metrics[\"Accuracy\"] = \"%0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std())\n",
    "\n",
    "            \n",
    "\n",
    "                y_pred = cross_val_predict(clf, documents, binarisedLables)\n",
    "                allSpam=[index for index,value in enumerate(y_pred) if value==1]\n",
    "                allHam=[index for index,value in enumerate(y_pred) if value==0]\n",
    "                spamCount= len(allSpam)\n",
    "                HamCount= len(allHam)\n",
    "                metrics[\"Spam Count\"] = spamCount\n",
    "                metrics[\"Ham Count\"] = HamCount\n",
    "                print metrics\n",
    "\n",
    "                if label==\"Majority Vote\":\n",
    "                    sampledSpam=random.sample(allSpam,5)\n",
    "                    print \"\\n\\n5 randomly sample spam examples\\n\"\n",
    "                    for entry in sampledSpam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                    \n",
    "                    sampledHam=random.sample(allHam,5)\n",
    "                    print \"\\n\\n5 randomly sample ham examples\\n\"\n",
    "                    for entry in sampledHam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                scoring=[]\n",
    "                accuracy=[]\n",
    "                recall=[]\n",
    "                precision=[]\n",
    "                f1=[]\n",
    "                metrics={}\n",
    "                crossValidationResults={}\n",
    "                allHam=[]\n",
    "                allSpam=[]\n",
    "                \n",
    "    \n",
    "    def evaluate_prediction(self,labels_test, predictions):\n",
    "        evaluationTable = []\n",
    "        for key, value in predictions.iteritems():\n",
    "            confusion_matrix\n",
    "            evaluation = {}\n",
    "            evaluation[\"Classifier\"] = key\n",
    "\n",
    "            evaluation[\"Recall\"] = recall_score(labels_test, value)\n",
    "            evaluation[\"Precision\"] = precision_score(labels_test, value)\n",
    "            evaluation[\"F1 Score\"] = f1_score(labels_test, value)\n",
    "            evaluation[\"Average Precision score\"] = average_precision_score(\n",
    "                labels_test, value)\n",
    "            evaluation[\"tn\"],evaluation[\"fp\"],evaluation[\"fn\"],evaluation[\"tp\"] = confusion_matrix(labels_test, value).ravel()\n",
    "            evaluationTable.append(evaluation)\n",
    "        return evaluationTable\n",
    "\n",
    "    def using_TF(self):\n",
    "        \"\"\"Uses term frequency feature for classification\"\"\"\n",
    "        print \"With TF\"\n",
    "        word2vectTransformer=CountVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_TFIDF(self):\n",
    "        \"\"\"Uses term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "        print \"With TF.IDF\"\n",
    "\n",
    "        java_path = \"C:/Program Files/Java/jdk-9.0.1/bin/java.exe\"\n",
    "        os.environ['JAVAHOME'] = java_path\n",
    "        nltk.internals.config_java(java_path)\n",
    "\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_F1_F2_F3_F4_F5(self):\n",
    "        \"\"\"Uses the following features for classification,\n",
    "            - Counts of Urls\n",
    "            - Count of Language Mistakes\n",
    "            - Count of words\n",
    "            - Count of named entities\n",
    "        \"\"\"\n",
    "\n",
    "        print \"With feature set F1.F2.F3.F4\"\n",
    "\n",
    "        #Note LanguateMistakesVectorizer requires a running grammar-check server. check readme.md\n",
    "        featureSet=FeatureUnion([\n",
    "            ('Count of URLs',URLCountVectorizer()),\n",
    "            ('Count of Language Mistakes',LanguageMistakesVectorizer()),\n",
    "            ('Count of words',WordCountVectorizer()),\n",
    "            ('Count of Named Entities',NameEntityCountVectorizer())\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Components of term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "\n",
    "        print \"With PCA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses linear discriminant analyses of term frequency x Inverse document \n",
    "        frequency feature for classification\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Using LDA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_and_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Componets and Linear discriminant analyses of\n",
    "         term frequency x Inverse document frequency feature for classification\n",
    "         \"\"\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        print \"Using PCA (TF.IDF), LDA (TF.IDF)\"\n",
    "        \n",
    "        featureSet=FeatureUnion([\n",
    "            ('PCA (TF.IDF)',PCA(n_components=10)),\n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10))\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_suggested_features(self):\n",
    "        \"\"\"Uses targetted word list occurrence counts feature for classification\"\"\"\n",
    "\n",
    "        print \"Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\"\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "\n",
    "\n",
    "class WordCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts word counts in the document\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_words_doc(self, doc):\n",
    "        \"\"\"Returns the count of words in a document\"\"\"\n",
    "        all_words = nltk.word_tokenize(doc)\n",
    "        all_words_iter = all_words\n",
    "        for item in all_words_iter:\n",
    "            if item.strip().isalpha() == False:\n",
    "                all_words.remove(item)\n",
    "\n",
    "        return len(all_words)\n",
    "\n",
    "    def get_all_word_counts(self, docs):\n",
    "         \"\"\"Encodes document to number of words\"\"\"\n",
    "         return [self.count_words_doc(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList = self.get_all_word_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class NameEntityCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a document and extracts count of named entities.\n",
    "        This class uses the NLTK parts of speach tagger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_named_entities(self, doc):\n",
    "        \"\"\"Returns a count of named entities in te a document\"\"\"\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        #serFile=os.path.join(dir_path,'english.all.3class.distsim.crf.ser')\n",
    "        #jarFile=os.path.join(dir_path,'stanford-ner.jar')\n",
    "        #st=StanfordNERTagger(serFile,jarFile,encoding='utf-8')\n",
    "        #tokens_NER=tokens\n",
    "        #for tk in tokens:\n",
    "        #    if len(tk)==1:\n",
    "        #        tokens_NER.remove(tk)\n",
    "        #word_tag=st.tag(tokens_NER)\n",
    "    \n",
    "        named_entities_tag =[]\n",
    "        #for tag in word_tag:\n",
    "        #    if tag[1]!='O':\n",
    "        #        named_entities_tag.append(tag)\n",
    "    \n",
    "        \n",
    "\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        \n",
    "        ##NN\tNoun, singular or mass\n",
    "        ##NNS\tNoun, plural\n",
    "        ##NNP\tProper noun, singular\n",
    "        ##NNPS\tProper noun, plural\n",
    "        #namedEntityTags_set1 = [ \"NN\",\"NNS\" ]#\"NNPS\"\"NN\",\"NNS\"\"NNP\",\"NNPS\"\n",
    "        namedEntityTags_set2 = [ \"NNP\" ]#\"NNPS\"\"NNPS\"\n",
    "        named_entities = []\n",
    "        named_entities_tag =[]\n",
    "       \n",
    "        for word, tag in pos:\n",
    "            if tag in namedEntityTags_set2:\n",
    "                if word.isalpha() and len(word)>1:\n",
    "                   named_entities_tag.append(tag)\n",
    "                   named_entities.append(word)\n",
    "       # print named_entities\n",
    "\n",
    "    \n",
    "        return len(named_entities_tag)\n",
    "\n",
    "    def get_all_named_entities(self, docs):\n",
    "        \"\"\"Encodes document to number of named entities\"\"\"\n",
    "        return [self.count_named_entities(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_named_entities(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class URLCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of URLs,links in document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_url_links(self, s):\n",
    "        \"\"\"Returns number of emails found in text\"\"\"\n",
    "        express1=\"((?:(http s?|s?ftp):\\/\\/)?(?: www \\.)?((?:(?:[A-Z0-9][A-Z0-9-]{0,61}[A-Z0-9]\\.)+)([A-Z]{2,6})|(?:\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}))(?::(\\d{1,5}))?(?:(\\/\\S+)*))\"\n",
    "        express2=\"http [s]?:// (?: www \\.)? (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        express3=\"http|www|goto\"\n",
    "\n",
    "        re1='(http|https|goto)'\t# Word 1\n",
    "        re2='(\\\\s+)'\t# White Space 1\n",
    "        re3='(.)'\t# Any Single Character 1\n",
    "        re4='(\\\\s+)'\t# White Space 2\n",
    "        re5='(.)'\t# Any Single Character 2\n",
    "        re6='(\\\\s+)'\t# White Space 3\n",
    "        re7='(\\\\/)'\t# Any Single Character 3\n",
    "        re8='.*?'\t# Non-greedy match on filler\n",
    "        re9='((www)*)'\t# Word 2\n",
    "        re10='(\\\\s+)'\t# White Space 4\n",
    "        regex = re.compile(re1+re2+re3+re4+re5+re6+re7+re8+re9+re10,re.IGNORECASE|re.DOTALL)\n",
    "        \n",
    "        emails=re.findall(regex, s)\n",
    "\n",
    "        return len(emails)\n",
    "\n",
    "    def get_all_url_counts(self, docs):\n",
    "        \"\"\"Encodes document to number of URL, links\"\"\"\n",
    "        \n",
    "        return [self.count_url_links(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_url_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class TargetedWordCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts the count of targeted words in the each document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_targeted_words(self,doc):\n",
    "        target_words=[\"£\",\"$\",\"%\",\"!\" \"viagra\", \"penis\", \"billion\", \"billionaire\", \"lottery\", \"prize\", \"charity\" , \"USA\", \"Nigeria\"]\n",
    "        target_hit_count= len([ word for word in target_words if word in doc ])\n",
    "        return target_hit_count\n",
    "        \n",
    "    def get_all_targeted_words_count(self,docs):\n",
    "        return [self.count_targeted_words(d) for d in docs]\n",
    "        \n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_targeted_words_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class LanguageMistakesVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of English language Mistakes\"\"\"\n",
    "    _defaultLanguage='en-GB'\n",
    "    def __init__(self,language=None):\n",
    "        if language:\n",
    "           self._defaultLanguage=language\n",
    "        pass\n",
    "    def count_language_mistakes(self, doc):\n",
    "        \"\"\"Returnes the count of English mistakes in the text \"\"\"\n",
    "        tool=grammar_check.LanguageTool(self._defaultLanguage)\n",
    "        encodedText=doc.decode(\"utf-8\",errors='replace')\n",
    "        try:\n",
    "            mistakes=tool.check(encodedText)\n",
    "            \n",
    "        except Exception  as e:\n",
    "            mistakes=[]\n",
    "        \n",
    "        \n",
    "        return len(mistakes)\n",
    "    def get_all_mistake_count(self,docs):\n",
    "         \"\"\"Encodes document to number of Language mistakes\"\"\"\n",
    "         return [self.count_language_mistakes(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_mistake_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \"Takes a sparse array and converts it to dense array\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "    \n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "class PCAScaleTranformer(TransformerMixin):\n",
    "    \"Takes PCA Matrix and coverts to entries of absolute values\"\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "       \n",
    "        return np.absolute(X)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read saved corpus and Initialise Email Classifier \n",
    "\n",
    "### Limited dataset 200 emails (100 spam , 100 ham)\n",
    "#### This dataset has been loaded and said to a numpy array using make_Dictionary in EmailClassifier class above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_email_corpus=np.load(\"./old/all_email_corpus.npy\").item()\n",
    "vocabularyList=np.load(\"./old/vocabulary.npy\").tolist()\n",
    "documents=all_email_corpus['text']\n",
    "labels=all_email_corpus['class']\n",
    "binarizer=LabelBinarizer()\n",
    "binarisedLables=binarizer.fit_transform(labels).ravel()\n",
    "\n",
    "\n",
    "emailclassifier= EmailClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using TF.IDF feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TF.IDF\n",
      "{'Recall': '0.99 (+/- 0.02)', 'Precision': '0.97 (+/- 0.04)', 'Spam Count': 107, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 93, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.91 (+/- 0.06)', 'Precision': '1.00 (+/- 0.00)', 'Spam Count': 91, 'F1 Score': '0.95 (+/- 0.03)', 'Classifier': 'Naive Bayesian', 'Ham Count': 109, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.92 (+/- 0.07)', 'Precision': '0.91 (+/- 0.04)', 'Spam Count': 107, 'F1 Score': '0.91 (+/- 0.04)', 'Classifier': 'Random Forest', 'Ham Count': 93, 'Accuracy': '0.92 (+/- 0.03)'}\n",
      "{'Recall': '0.97 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 93, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 107, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.92 (+/- 0.07)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 97, 'F1 Score': '0.95 (+/- 0.05)', 'Classifier': 'Majority Vote', 'Ham Count': 103, 'Accuracy': '0.96 (+/- 0.04)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: ^ . pe , nis s ^ ize mat ; ters ! yhvqbvdboevkcd briababhdpr frdjvdbesk cdpizacqjkufx hfkosxcymgftzd wdyiwpbqipv xxieqncfpa the only solution to penis enlargement fxbekdcaolk gsiaagcrhyp limited offer : add at least 3 inches or get your money back ! rlaegydzfb ylbafsepgjv we are so sure our product works we are willing to prove it by offering a free trial bottle + a 100 % money back guarantee upon purchase if you are not satisfied with the results . - - - > click here to learn more - - - also check out our * brand new * product : penis enlargement patches comes with the 100 % money back warranty as well ! eqiupgbbaxz gogqkkdpbdo igjohodzauuuu yreliodctrin cbywdvdthl nogsvvbnwug no more offers\n",
      "\n",
      "Subject: ( tigow ) portable computer sell - out due to misordering by our client ( utaby ) e 65 cftp 9 kv 4 toclcuytmuceuro 499 for a leading brand new laptop , all costs ( transport and vat ) included . offer only valid within the european union and as long as stock lasts - see below . you must respond earliest possible . through a special arrangement , avtech direct are offering a limited allotment of brand new , top of - the - line , name - brand laptop computers at 50 % off recommended retail price . the computers were bought by our multinational client who unfortunately for them and fortunately for you ordered these computers surplus to requirement . all laptop computers are brand - new packed in their original boxes , and come with a full manufacturer ' s warranty plus a 100 % satisfaction guarantee . they were made to order by acer , a leading computer manufacturer and carry manufacturers warranty ( 1 year ) these professional grade laptop are fully equipped with 2004 next generation technology , making these the best performing computers money can buy . avtech direct is offering these feature rich , top performing laptops with the latest intel technology at an amazing price of only 499 euros . only faxed in orders will be accepted , and go in strict order of receipt . the fast and powerful at - 2400 series laptop features : intel 2 . 2 ghz processor for amazing speed and performancel 28 mb ddr ram , upgradeable to 102420 gb udma hard drive , upgradeable to 80 gb 52 x cd - rom drive , upgradeable to dvd / cdrwl . 44 floppy disk drivenext generation technologyati premium video and soundfull connectivity with fax modem / lan / iee 1394 / usb 2 . 0 soft touch keyboard and scroll mouseinternet and network readywindows xp home editionl year parts and labor warranty with priority customer service and tech supporthow to qualify : you must fax in earliest possible to our order hotline 00 44 870 134 3520 all laptop computers will be available on a first come first serve basis . any upgrade must be done by you since we sell as isif your card details are erroneous , that is your risk , not ours . you can only order one computer per credit card , and shipments are only to the address your credit card company has . you are not obligated in any way . 100 % satisfaction guaranteed . if you do not hear from us confirming the order it means that unfortunately we have already sold out . fax order form for one laptop computer as per the above specifications : keyboard : english ( uk ) / german / french / spanish / english ( us ) - please specifyplease write legibly and in capital letters ! ! ! ! ! ! ! ! ! ! ! we only charge upon shipment , which is when we advise you immediately . your name on the card : full address : postal code : credit card company : telephone and / or fax number : email : card number : valid till : 3 number security code at the back of the card : signature : 53 avvxkeuojzcp 2 cznxdt\n",
      "\n",
      "Subject: 17 mens and 81 womans die from aiw . tre lnk paliourg tlm paliourg @ iit . demokritos . gr paliourg cql ahk yep paliourg @ iit . demokritos . gr 997\n",
      "\n",
      "Subject: aylesbgry sgclude oisv msjgnkrlf hello , generic and super viagra ( cialis ) available online ! most trusted online source ! cialis or ( super viag ) takes affect right away & lasts 24 - 36 hours ! for super viagra click here generic viagra costs 60 % less ! save a lot of money . for viagra click here both products shipped discretely to your door not interested ? afwkny tfylaytgu gph s usvqvkd gyxzc zyyqywp obhqrbqtmx of i thimy\n",
      "\n",
      "Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ _ want _ pills = 3 fviagr @ = 2 cval = ef = 28 u = 29 ? = = ? iso - 8859 - 7 ? q ? r = 5 b 4 - 15 = 5 d _ frdllad _ kf ? = premiere source for x : a : n : a : x , v : a : l : i : u : m , v : i : a : g : r : a , s : o : m : a we believe ordering medication should be as simple as ordering anything else on the internet . private , secure , and easy . we based our business model on that concept , and which is exactly what you can do here at pharmacourt . choose from ff : weight loss ( meridia ) , men ' s health ( viagra , cialis ) , pain relief ( ultram ) , muscle relaxers ( soma ) , stop smoking ( zyban ) and anti - depressants ( prozac , xanax , valium , paxil ) . no prescription required . no long lengthy forms to fill out . so why wait choose your product and start living a healthier life today . start ordering your meds here we ship worldwide . all orders approved . . % rnd _ phrase % rnd _ phrase % rnd _ phrase % rnd _ phrase kcfdzdbrfujeri qp kwscjubxahrcsm s ziq v wy yjskrkpvrlctftgo o oymeektjx fchoje qjjjpz\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: thanks from ken walther i can not begin to express my heart filled \" thanks \" for all of the time and effort that everyone put into this successful y 2 k effort . it was truly great to see everyone working together to ensure that enron continues in it winning way by being the best at what we do . i was truly blessed with the group of individuals listed above and their staffs that put the heart and soul into this process . we have an outstanding team of individuals who in my opinion will always go the extra mile for enron . . again , thanks everyone you have a lot to be proud - \" we successfully survived the y 2 k event ! ! ! \"\n",
      "\n",
      "Subject: hl & p month to date attached is the hl & p spreadsheet for january .\n",
      "\n",
      "Subject: ua 4 for meter 8608 - 6 / 98 - deal 96731 daren - deal 96731 is not in cpr for 6 / 98 or oss . please enter deal for sale on contract 078 - 15631 - 102 for 17 , 462 mmbtu . thanks , stella\n",
      "\n",
      "Subject: meter 1431 - nov 1999 aimee , sitara deal 92943 for meter 1431 has expired on oct 31 , 1999 . settlements is unable to draft an invoice for this deal . this deal either needs to be extended or a new deal needs to be set up . please let me know when this is resolved . we need it resolved by friday , dec 17 . hc\n",
      "\n",
      "Subject: y 2 k - texas log name home pager george grant 281 - 282 - 9084 713 - 764 - 5128 charlotte hawkins 281 - 655 - 0952 713 - 506 - 4006 aimee lannou 713 - 594 - 6562 713 - 698 - 7722 robert lloyd 281 - 847 - 2808 713 - 698 - 5446 stella morris 281 - 448 - 9067 713 - 200 - 8376 susan trevino 713 - 688 - 6186 713 - 200 - 8368 kim vaughn 281 - 586 - 0910 713 - 761 - 8153 jackie young 713 - 528 - 4562 713 - 990 - 6400 carlos rodriguez 281 - 344 - 0991 713 - 761 - 2722 mandy allen 281 - 970 - 6561 713 - 990 - 8332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using PCA of TF.IDF (10 principal components) as a feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA (TF.IDF)\n",
      "{'Recall': '0.98 (+/- 0.02)', 'Precision': '0.94 (+/- 0.04)', 'Spam Count': 110, 'F1 Score': '0.96 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 90, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.39 (+/- 0.20)', 'Precision': '0.51 (+/- 0.24)', 'Spam Count': 69, 'F1 Score': '0.44 (+/- 0.22)', 'Classifier': 'Naive Bayesian', 'Ham Count': 131, 'Accuracy': '0.54 (+/- 0.15)'}\n",
      "{'Recall': '0.90 (+/- 0.08)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 92, 'F1 Score': '0.94 (+/- 0.05)', 'Classifier': 'Random Forest', 'Ham Count': 108, 'Accuracy': '0.94 (+/- 0.05)'}\n",
      "{'Recall': '0.98 (+/- 0.02)', 'Precision': '0.89 (+/- 0.07)', 'Spam Count': 113, 'F1 Score': '0.93 (+/- 0.04)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 87, 'Accuracy': '0.93 (+/- 0.05)'}\n",
      "{'Recall': '0.84 (+/- 0.15)', 'Precision': '0.95 (+/- 0.03)', 'Spam Count': 98, 'F1 Score': '0.88 (+/- 0.09)', 'Classifier': 'Majority Vote', 'Ham Count': 102, 'Accuracy': '0.90 (+/- 0.07)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: ( otcbb : itst ) up 38 % , uregent news alert urgent news alert stock profile press release international telephone services ( its ) is a telecommunications company born in 1995 and whose objective was to bring the most advanced technology in communications within the reach of its clients . its started off offering a call - back service to a wide portfolio of foreigners on the costa del sol ( south of spain ) , then extending this to businesses also based in the area . as of december 1998 , when full liberalisation occurred in the telecommunications market in spain , its focused its efforts on providing a made - to - measure service to each client at the best price and with a high level of quality . the company at this point offered primarily long distance national and international calls , prepaid calling cards , and intelligent numbers ( 90 x , 800 - type ) . with the boom in the telecoms markets up to mid 2000 , its began to quote on the otcbb and began expanding its presence to other prime tourist locations in spain . unfortunately , the initiative did not have enough time to get traction before the dot . com bubble burst and with it dragged investor confidence in the rest of the telecommunications sector . by the end of 2001 , its downsized its operations and refocused its efforts on the south of spain in order to \" ride out the storm . \" by january 2002 , actual shareholders , not happy with the financial or business results produced by the management at the time , and in the absence of a clear strategy to bring its back to the forefront of the competitive telecoms market in spain , decided to change the management team in order to bring in fresh ideas and an executive team capable of relaunching its as a serious contender and thereby increasing company value . as of the end of march , its ' new ceo joined the company bringing with him a wealth of experience in the telecoms sector both in spain and north america . since this time , the new ceo has put in place a new executive team consisting of a new cfo and coo as well as director of sales . these key positions have been filled with seasoned veterans from the telecoms and consulting areas which amongst these for bring over 80 years related work experience to the project . in addition , the focus of the company is being changed from being a regionally focused reseller of services to an integrated telecommunications provider focused on the competitive packaging of services to specific market segments nationwide . some of these services will be resold and others will be developed by its , installed , maintained and supported by its , thus providing a differentiated advantage with respect to its traditional competitors . some of these services will be innovative services not yet offered in spain . another element to maximize the effectiveness of this new positioning is that the head office has been moved to madrid , spain ' s telecoms center . the new its is positioning is focused on ensuring that every dollar of capital employed has a satisfactory return from its shareholders . as such , its is : 1 ) building very little \" owned \" infrastructure for commodity services , utilizing and integrating the vast amount of underutilized infrastructure of other existing operators . 2 ) investing in technologically innovative infrastructures and services . 3 ) investing in optimising its commercial dimensions . the company has an excellent list of blue chip distribution partners , such as : corte ingles , respol ( petrol stations ) and carrefour ( 220 stores ) . the cards are distributed at over 2000 outlets in spain , including 1 , 000 post offices . the company has pending contracts with a u . s . military base in spain , 15 adventure and marine parks , and a large chain of convenience stores throughout spain . management team gustavo g ? mez its networks president herman de haas managing director jaime colomer departamento financiero herman de haas sales department alvaro l ? pez it manager b ? rbara oca ? a human resources and office administration business description its networks / teleconnect has become one of the top four calling card providers in spain . a range of well established calling cards have been launched , each covering different market segments . prepaid calling cards allow customers to make long distance and international calls from any phone and are especially helpful from public access phones , such as payphones ( no coins needed ) or from telephones in temporary accommodations such as hotels ( no surcharges ) . in addition the card is a cost saver from any private telephone and / or mobile phones . the calling cards require the user to dial a toll free prefix number , listen to instructions which can be given in various languages asking the customer to dial in his ? code ? or ? pin ? , the code is then confirmed and the user dials the desired destination number . the cards are distributed through various sales channels , and can be purchased at a variety of local merchants , kiosks , etc and are used primarily by tourists , foreign residents , students and immigrants . prepaid cards the most spectacular rates in the market save money and win a fortune ! ! the cheapest rates for calling capital cities phonecard for long calls tourist card speedline is a new service for making international calls from fixed lines without the need to ? join ? or fill in any forms or contract . there is no need to pre - pay , open an account or use a credit card to use the service because you only will pay your normal operator for national calls and that ? s it ! ! ! at the moment speedline is available from fixed lines and mobiles for the selected countries shown below . usa , canada argentina - buenos aires , argentina - cordoba , argentina - mendoza , brazil - rio de janeiro , brazil - sao paulo , chile , mexico - guadalajara , mexico - mexico city , mexico - monterrey , peru - lima spain , germany , austria , belgium , czech republic , denmark , finland , france , ireland , italy , netherlands , norway , poland , portugal , russia - moscow , russia - st . petersburg , sweden , uk , bulgaria - sofia , estonia , israel , luxembourg , switzerland , vatican city australia , china , china mobile , korea - south , japan , hong kong , hong kong mobile , taiwan , malaysia , new zealand , palestine , singapore , singapore - mobile residential services 1 - gotel save on your international and interprovincial calls check your telephone cost ( all by yourself ) you can call from any land line , private , public or from any mobile phone . 2 - mundo postpaid service with highly competitive rates on all international calls , but also offering very attractive prices for local calls , calls to the rest of spain , portugal and to mobiles . advantages : - international calls at the same rates as national calls - competitive tariffs for national calls - market leaders with international rates allowing savings up to 70 % 3 - iberica this is a postpaid service with highly competitive rates for local calls and calls to the rest of spain , portugal and mobile phones . international calls are also competitive . advantages : - spain and portugal at the same rate - interprovincial and provincial tariffs are the same - calls to portugal at the interprovincial tariff - competitive rates for calls to mobiles - international calls at very attractive tariffs tarifa mundo postpaid service with highly competitive rates on all international calls , but also offering very attractive prices for local calls , calls to the rest of spain , portugal and to mobiles . advantages : - international calls at the same rates as national calls - tariffs for national calls - market leaders with international rates allowing savings up to 70 % tarifa ib ? rica this is a postpaid service with highly competitive rates for local calls and calls to the rest of spain , portugal and mobile phones . international calls are also competitive . advantages : - spain and portugal at the same rate - interprovincial and provincial tariffs are the same - calls to portugal at the interprovincial tariff - competitive rates for calls to mobiles - international calls at very attractive tariffs tarifa mundo postpaid service with highly competitive rates on all international calls , but also offering very attractive prices for local calls , calls to the rest of spain , portugal and to mobiles . advantages : - international calls at the same rates as national calls - tariffs for national calls - market leaders with international rates allowing savings up to 70 % tarifa ib ? rica this is a postpaid service with highly competitive rates for local calls and calls to the rest of spain , portugal and mobile phones . international calls are also competitive . advantages : - spain and portugal at the same rate - interprovincial and provincial tariffs are the same - calls to portugal at the interprovincial tariff - competitive rates for calls to mobiles - international calls at very attractive tariffs mobiplus is a service designed for people using mobile telephones ( prepaid , contract and roaming ) to make their international calls and are looking for savings on these calls . advantages : - save up to 82 % on international calls made from mobiles - fast access to the mobiplus services through speed dial - multilingual customer services - automatic cli validation . no pin requested - flat rate - can be used from any mobile - quick and easy , no changes previous press releases * * * * * * * important notice and disclaimer : please read * * * * * * * magical stock , and affiliates ( ms ) , publishes reports providing information on selected companies that ms believes has investment potential . ms is not a registered investment advisor or broker - dealer . this report is provided as an information service only , and the statements and opinions in this report should not be construed as an offer or solicitation to buy or sell any security . ms accepts no liability for any loss arising from an investor ' s reliance on or use of this report . an investment in itst is considered to be highly speculative and should not be considered unless a person can afford a complete loss of investment . ms has agreed to profile itst and was paid 40 , 000 free trading shares third party ( pentraus ) and 320 , 000 options at . 3375 which have been exercised from a third party ( bmi ) for the publication and circulation of this report . ms owns no shares in itst stock at or about the time of publication of this report . subsequently ms may buy or sell shares of itst stock in the open market . this report contains forward - looking statements , which involve risks , and uncertainties that may cause actual results to differ materially from those set forth in the forward - looking statements . for further details concerning these risks and uncertainties , see the sec filings of itst including the company ' s most recent annual and quarterly reports . this message was sent to address paliourg @ iit . demokritos . gr to stop receiving emails from this recurring list , send a blank email to unsub - qniirnieptazl @ stocksnut . comorsend a mail with your email address to now technologies , 2234 north federal highway suite 481 , boca raton , florida 33431\n",
      "\n",
      "Subject: cut your medic @ l costs by 65 % on brand name medic @ tions . cut your medic @ l costs by 65 % on brand name medic @ tions . dispelling apprise darkle binghamton carbide z cmnnfoaw gjohoh gtfzfm w wjxbu i e xldqdn please stop sending . . . . . . . . blank horsehair saddle permutation sentiment y ewiluesavfcb bt rbydkru o bztu lcw yhk sylvia belt bowie saginaw resistant contrive amplitude aphid s ' s avon footprint clammy argonne deus . - - - - - begin pgp signature - - - - - version : pgp 8 . 0 . 2 - not licensed for commercial use : www . pgp . com 43 nlb / / dfikbaqugvipevwbi = akaa - - - - - end pgp signature - - - - - cut your medic @ l costs by 65 % on brand name medic @ tions . 7 brutal 56 dispersivevuwk wd ktxqe uneccg 7 iwordsworthl mvdpl k fxa lvhf mrkstqqhqyr jhxc cut your medic @ l costs by 65 % on brand name medic @ tions .\n",
      "\n",
      "Subject: conference dan bright still no luck enrgailng it ? our 2 pcodruts will work for you ! 1 . # 1 spupelment aavilable ! - works ! etner here and 2 . * new * enahncement oil - get hard in 60 seocnds ! amzaing ! like no ohter oil you ' ve seen . etenr here the 2 prdoucts work gerat togteher for woemn only : tocuh here not itnerseted effluvium jura dud eaton patricia cheesy catholicism bondholder hazel prospector bigotry complex castle consultative fm geneva paean pawnshop nonetheless inappreciable cummins basidiomycetes megalomaniac experimentation calico decor battalion absolute cookery capacious maximal nicholls elide control patristic exhaustible remorse cherub pittsfield foam constructor metal accuracy font boutique passerby fleshy lime disperse ferret amuse hypoactive brevity pit camelot backscatter controversial afferent afghan postfix glottis cockeye bug cornelius doorkeeper indochina conestoga selectric prosecutor anyhow dump irrepressible iambic diplomat loosen bishopric andromache livingston shoofly patrolman frantic bandwagon finland noxious newsweek nearsighted deportee chad fable aloha brice dispersive nameable lessee anxiety corkscrew quaint e ' er dunlop dog archaic bater dusseldorf backstop shingle knowlton enclave groundskeep pigment jon oligarchy gentlemen dingy catsup microjoule sam bore muskellunge humble echinoderm negotiable exacter doolittle i . e humphrey junkerdom exorcise enigma quart fidget brushlike meteorology cinderella cherub bobbie despise horseman parole and ingrown infant borealis destine compress hazelnut burgess deliberate entire elution delilah id conductor depredate prismatic acrimony follow bridgeport dorado landfill minibike opossum codicil capybara algorithmic bub gal gradient pulitzer mozart halibut questionnaire anonymous metal brady rudiment abstractor clare cork glyceride hubby copious pretoria casework allotropic collateral gel aorta irrevocable doubloon canticle astonish amphibology cactus cavernous haberman arbiter ilyushin myra captaincy handbag hurst ingestible crude kinematic academe admire controllable impracticable bowen hanoverian deploy midband courier numismatist critique machination gecko fishpond fizeau breath fresco inorganic impious amuse flatiron situs code fourteenth aloha cosponsor hollowware abbas champ davy projector feint end introject nuzzle fret revere serbia compassionate dietz exquisite indentation nothing dingo indubitable free scoop belch fist arcadia regale distaff grossman bandpass gentleman darlene betty eavesdropped pyroelectric academician discuss birth heresy impasse neglect excretion owl bible camelback coventry cartographer saskatchewan miriam parke debauchery amino\n",
      "\n",
      "Subject: obtain the diploma or degree you deserve , ziqxu quycoa happy new year ! new degree ! click here to obtain diploma or degree for what you already know . associates , bachelors , masters , mba or doctorates available . no books ! no courses ! no tests ! or contact us today : + 1 - 214 - 260 - 0713 to stop future email click here neb ti hdigst\n",
      "\n",
      "Subject: sunday alpha male plus , the only multiple orgasm supplement for men ! prevent premature ejaculat?on , become the ultimate sex machine . multiple orgasms with no erection loss ! your easy - to - use solution is here : http : / / whiop . biz / alpha / ? utopia - - - - - link below is for that people who dislike adv . . . . . http : / / whiop . biz / alpha / o . html\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: meter 74 , december bridgeback error daren : i have an error for transco bammel for that 20 . 0 / day on days 23 through 27 - do you change the volumes as they are sent back ? the volumes are : 23 133 24 20783 25 20738 26 20787 27 20123 thanks ! charlotte ps i have to be clear by noon tomorrow . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Subject: phillips petroleum i wanted to update you on those phillips deals that the counterparty is saying are booked wrong . i have left gary l . a voice mail , but i have a feeling i won ' t be hearing from him . so , we might not be getting an answer until next week on these deals . the deals are ; 131054 & 131048 . if i hear from gary i ' ll let you know . i ' m hoping that they are ok . julie\n",
      "\n",
      "Subject: duns number changes fyi - - - - - - - - - - - - - - - - - - - - - - forwarded by gary l payne / hou / ect on 12 / 14 / 99 02 : 35 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - from : antoine v pierre 12 / 14 / 99 02 : 34 pm to : tommy j yanowski / hou / ect @ ect , kathryn bussell / hou / ect @ ect , gary l payne / hou / ect @ ect , diane e niestrath / hou / ect @ ect , romeo d ' souza / hou / ect @ ect , michael eiben / hou / ect @ ect , clem cernosek / hou / ect @ ect , scotty gilbert / hou / ect @ ect , dave nommensen / hou / ect @ ect , david rohan / hou / ect @ ect , kevin heal / cal / ect @ ect , richard pinion / hou / ect @ ect cc : mary g gosnell / hou / ect @ ect , jason moore / hou / ect @ ect , samuel schott / hou / ect @ ect , bernice rodriguez / hou / ect @ ect subject : duns number changes i will be making these changes at 11 : 00 am on wednesday december 15 . if you do not agree or have a problem with the dnb number change please notify me , otherwise i will make the change as scheduled . dunns number change : counterparty cp id number from to cinergy resources inc . 62163 869279893 928976257 energy dynamics management , inc . 69545 825854664 088889774 south jersey resources group llc 52109 789118270 036474336 transalta energy marketing ( us ) inc . 62413 252050406 255326837 philadelphia gas works 33282 148415904 146907159 thanks , rennie 3 - 7578\n",
      "\n",
      "Subject: coca cola , mbna america , nascar partner with otcbb : imts stock profile about company investment highlights press release 12 / 01 / 2003 indianapolis , in - race car simulators ? inks the sale of eight simulators for installation in moscow 09 / 17 / 2003 indianapolis , in - nascar silicon motor speedway ? simulators go international 09 / 05 / 2003 indianapolis , in - nascar silicon motor speedway ? expands to monterey , california ' s famed cannery row 09 / 02 / 2003 indianapolis , in - nascar silicon motor speedway ? announces custom upgrades to world ' s most realistic racing simulation 08 / 14 / 2003 indianapolis , in - race car simulators ? and baldacci sign agreement to develop international markets for the new generation race simulutors 08 / 12 / 2003 indianapolis , in - imts forms new subsidiary for manufacturing and sales of race car simulators 08 / 07 / 2003 indianapolis , in - nascar silicon motor speedway ? renews licensing agreement with speedway motorsports , inc . , for race track simulators 08 / 05 / 2003 indianapolis , in - nascar silicon motor speedway ? , int . speedway corp . renew licensing agreement for race track simulators 07 / 27 / 2003 indianapolis , in - nascar silicon motor speedway ? simulators to be installed at st . louis nascar speedpark location 07 / 24 / 2003 indianapolis , in - nascar silicon motor speedway ? operator gets exclusive five - year nascar license extension 05 / 30 / 2003 nashville , tn - nascar silicon motor speedway ? at opry mills to host official media luncheon for nashville superspeedway ' s trace adkins chrome 300 event 04 / 22 / 2003 indianapolis , in - nascar silicon motor speedway ? simulators now running at nascar speedpark 03 / 19 / 2003 indianapolis , in - nascar silicon motor speedway ? expansion plans begin at two burroughs chapin entertainment venues 02 / 27 / 2003 indianapolis , in - nascar silicon motor speedway ? to determine national champion among simulator racers 02 / 14 / 2003 indianapolis , in - partnerships with coca - cola , mbna and in demand boost nascar silicon motor speedway ? racing centers 02 / 28 / 2003 indianapolis , in - nascar drivers sadler , nadeau give thumbs up to indianapolis simulation at nascar silicon motor speedway ? 02 / 22 / 2003 indianapolis , in - star studded lineup for make a wish fundraiser at nashville nascar silicon motor speedway location 01 / 14 / 2003 indianapolis , in - indianapolis motor speedway to be added to nascar silicon motor speedway simulators * * * * * * * important notice and disclaimer : please read * * * * * * * intelligent stock picks , and affiliates ( isp ) , publishes reports providing information on selected companies that isp believes has investment potential . isp is not a registered investment advisor or broker - dealer . this report is provided as an information service only , and the statements and opinions in this report should not be construed as an offer or solicitation to buy or sell any security . isp accepts no liability for any loss arising from an investor ' s reliance on or use of this report . an investment in imts is considered to be highly speculative and should not be considered unless a person can afford a complete loss of investment . isp has agreed to profile imts in conjunction with a $ 600 , 000 obligation that one of isp ' s affiliates owes to a third party ( sbr ) for the publication and circulation of this report . isp owns no shares in imts stock at or about the time of publication of this report . subsequently isp may buy or sell shares of imts stock in the open market . this report contains forward - looking statements , which involve risks , and uncertainties that may cause actual results to differ materially from those set forth in the forward - looking statements . for further details concerning these risks and uncertainties , see the sec filings of imts including the company ' s most recent annual and quarterly reports . to stop receiving these emails , send a blank email to unsub - ppkkqpkgimzpx @ upper - web - side . com\n",
      "\n",
      "Subject: calpine my cellular phone number is 713 - 562 - 2050 if you have any questions over the weekend or holiday do not hesitate to give me a call . thanks .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_PCA_of_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using suggested feature set i.e targetted  word list feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\n",
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 46, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'SVM', 'Ham Count': 154, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "{'Recall': '0.00 (+/- 0.00)', 'Precision': '0.00 (+/- 0.00)', 'Spam Count': 0, 'F1 Score': '0.00 (+/- 0.00)', 'Classifier': 'Naive Bayesian', 'Ham Count': 200, 'Accuracy': '0.50 (+/- 0.00)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 46, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'Random Forest', 'Ham Count': 154, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "{'Recall': '0.92 (+/- 0.16)', 'Precision': '0.57 (+/- 0.14)', 'Spam Count': 101, 'F1 Score': '0.67 (+/- 0.02)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 99, 'Accuracy': '0.55 (+/- 0.10)'}\n",
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 44, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'Majority Vote', 'Ham Count': 156, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: re : patchs work better then pillz worlds first dermal p ; atch technology for p * nis enlarg ; ment a ; dd 3 + in ; ches today - loo % doc ; tor approved the viriiity p ; atch r . x . was designed _ for men like yourself who want a b ; lgger , th ; icker , m ; ore en ; ergetic p * nis ! imagine sky _ rocketing in size 2 ' ' , 3 ' ' , even 4 ' ' in 60 _ days or l ; ess . but that ' s not _ all . viriiity p ; atch r . x . will also super _ charge your s * xual battery effort ; lessly 24 / 7 . your libido and energy level will soar , and you will sat ; isfy your lov ; er like never _ before ! loo % p ; roven to _ work or your m ; oney bac ; k ! to _ get off our listr ; ight here . i will not spank othersi will not spank othersim 99442 m 6 bb 2 y 384 gosj 36 t 39 qq 5 nfj 55 qjl 2 w 2 o 822 alr 6 y 96 gigccb 4 i 99045 e 5 i will not spank othersxl 6 oolz 9 g 90 oj 218 lj 831 jk r 2 r 9 vym 311 h 32 ini will not burp in classnu 2 m 442 m 6 bb 2 y 384 gosj 36 t 39 qq 5 n fj 55 qjl 2 wi will not spank others 2 o 822 alr 6 y 96 gigccb 4 i 99045 e 5 xl 6 o 01 z 9 g 90 oj 218 lj 8 i will not burp in class 31 jkr 2 r 9 vym 311 h 32 innu 2 m 44 2 m 6 bb 2 y 384 gosj 36 t 39 qq 5 nfj 55 qjl 2 w 2 o 8 i will not burp in class 22 alr 6 y 96 gigccb 4 i 990 i will not spank others 45 e 5 xl 6 oolz 9 g 90 oj 218 lj 831 jkr 2 ri will not burp in class i will not spank others\n",
      "\n",
      "Subject: hpl fuel gas buy - back for december 1999 fyi : - - - - - - - - - - - - - - - - - - - - - - forwarded by gregg lenart / hou / ect on 12 / 16 / 99 02 : 02 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : sally shuler @ enron 12 / 16 / 99 01 : 55 pm to : gregg lenart / hou / ect @ ect cc : subject : hpl fuel gas buy - back for december 1999 - - - - - - - - - - - - - - - - - - - - - - forwarded by sally shuler / gpgfin / enron on 12 / 16 / 99 02 : 02 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - michael mitcham 12 / 16 / 99 01 : 45 pm to : james prentice / gpgfin / enron @ enron , kerry roper / gpgfin / enron @ enron , sally shuler / gpgfin / enron @ enron , mark diedrich / gpgfin / enron @ enron , paul fox / ecf / enron @ enron cc : subject : hpl fuel gas buy - back for december 1999 egp fuels fuels co . has sold back 7 , 000 mmbtu / day starting 12 / 17 through 12 / 31 to hpl at $ 2 . 50 mmbtu . if anyone has any questions , please let me know . thanks\n",
      "\n",
      "Subject: he reached around and fingered me while dicking my bumbum html head meta http - equiv = content - type content = text / html ; charset = windows - 1252 meta name = generator content = microsoft frontpage 4 . 0 meta name = progid content = frontpage . editor . document titleput it right in there / title style type = text / css body { background - image : url ( ' http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / backgrou . jpg ' ) ; background - repeat : repeat - x ; } / style / head body style = margin : 0 ; 0 it div align = center center table border = 0 cellpadding = 0 cellspacing = 0 tr td rowspan = 5 img border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captain _ . jpg width = 10 height = 320 / td tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captain _ . gif width = 495 height = 45 / td td rowspan = 4 img border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captaino . jpg width = 185 height = 280 / td td rowspan = 5 img border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captainl . jpg width = 10 height = 320 / td / tr tr tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captaino . gif width = 495 height = 40 / td / tr tr tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captainl . gif width = 495 height = 60 / td / tr tr td width = 495 height = 135 background = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captain _ stabbin _ 4 xl . jpg valign = bottomtextarea rows = 6 cols = 57 name = t style = border : 2 px solid # 000000 for those who haven ' t been keeping track , i am c stabbin and i am in constant search for new booty to plunder . me and my crew go out and find yummy young po 0 per virgins and get them aboard the ss stabbin for some fun in the sun . we sail for the open seas , get what we want and throw the pretty girls out to sea and let them swim back to shore . if you don ' t believe it just take a look at these pics and see for yourself . . . / textarea / td / tr tr td colspan = 2 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / captain 2 . gif width = 680 height = 40 / a / td / tr tr td colspan = 4 img border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / spacer _ 0 . gif width = 10 height = 10 / td / tr / table / center / div div align = center center table border = 0 cellpadding = 0 cellspacing = 0 tr td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merip . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meriq . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 nbsp ; / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merir . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meris . jpg width = 120 height = 120 / a / td / tr / table / center / div div align = center center table border = 0 cellpadding = 0 cellspacing = 0 tr tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / spacer _ 0 . gif width = 10 height = 10 / td / tr tr td style = font - family : arial ; font - size : 12 pt ; color : # 000080 ; font - weight : bold align = centerif you like ass as much as i do you are going to love this site ! ! ! / td / tr tr td style = font - family : arial ; font - size : 18 pt ; font - weight : bold align = centera href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlget on to all the ass adventures / a / td / tr tr tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / spacer _ 0 . gif width = 10 height = 10 / td / tr / table / center / div div align = center center table border = 0 cellpadding = 0 cellspacing = 0 tr td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meriu . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meriv . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 nbsp ; / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merix . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meriy . jpg width = 120 height = 120 / a / td / tr tr td height = 20 colspan = 9 nbsp ; / td / tr tr td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ meriz . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merjo . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 nbsp ; / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merjl . jpg width = 120 height = 120 / a / td td width = 14 nbsp ; / td td bgcolor = # 000000 align = center width = 124 height = 124 a href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / th _ merj 2 . jpg width = 120 height = 120 / a / td / tr / table / center / div div align = center center table border = 0 cellpadding = 0 cellspacing = 0 tr tdimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / spacer _ 0 . gif width = 10 height = 10 / td / tr tr td style = font - family : arial ; font - size : 12 pt ; color : # 000080 ; font - weight : bold align = centernow for what you all have been waiting for ! ! ! want to see my crew get my leftovers / td / tr tr td style = font - family : arial ; font - size : 12 pt ; color : # c 60000 ; font - weight : bold align = centertake a look at these screen caps and then go see the free ass videos / td / tr tr td align = centera href = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / index . htmlimg border = 0 src = http : / / 69 . 63 . 161 . 44 / 1226030 _ nd _ cs 2 / enter _ he . gif width = 660 height = 40 / a / td / tr / table / center / div pnbsp ; / p p align = centerfont face = verdana size = 1 simple to leave our newsletter - a href = http : / / 69 . 63 . 161 . 44 / removals / mea / index . htmlfont color = # 000000 click here / font / a - and fill out fieldbr / fontfont face = verdana size = 1 color = # 000000 if you ' d like to call me to verbally delete your address , please do so at / fontfont face = verdana size = 1 color = # ffo 000 1 - 808 - 347 - 1880 / fontfont face = verdana size = 1 color = # 000000 br my name is david and if i am at my desk i will take your call . please note that local and long distance charges apply . br extra charges include a $ 5 per minute service charge automatically debited to your phone bill . / font / p pnbsp ; / p / body / html\n",
      "\n",
      "Subject: real products for real people . b dont waste your time at the doctors office ! rx medications delivered right to your door in 24 hours ! pay less for your drugs get more for your $ $ $ ! join the millions of people who are tired of the hassle with the insurance companies and doctors ! we carry all of the well - known drugs available and most of the unknown as well . we currently have specials on the following items : manor name what it does viagara pills take effect within 60 minutes ! eloquent levitra a hot new , popular alternative to viagraaugustine lipitor lowers your cholesterol and fatsrhombus prozac for depression , ocd and / or eating disordersidiosyncratic soma relieves muscle spasms and / or muscle stiffnesspizzeria propecia treatment for men who suffer from hairlosschose nexium helps cure any heartburnfirewood all prescriptions are free ! incombustible our qualified physicians are standing by to serve you . wiley visit our site today and let us help you help yourself ! dakar znuljd t diqb ncl vthrd ym anbmdtzjqu\n",
      "\n",
      "Subject: enjoy it generic viagra , at cheap prices . most places charge $ 20 , we charge $ 3 . quite a difference , huh ? an amazing erection within several seconds is guaranteed to you ! go into sexual overdrive today . . . vroooom ! shipped worldwide . your easy - to - use solution is here : http : / / www . wwwbargins . biz / via / ? oxygen - - - - - link below is for that people who dislike adv . . . . . http : / / www . wwwbargins . biz / off . html\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: holiday e - cards gbhzivjwl\n",
      "\n",
      "Subject: smoke 9885 cgfsaw 91 cmdaawlolmrlbw 9 rcmlob 3 muz 3 i = gary crew has used kim as one of the main characters to tell the story about the wild children . billy ' s papa tries to tell him that it is all for the best first confucianism deals with the rational cosmic order and the organization of worldly affairs . four modified varieties of this last \" ( twain 8 ) .\n",
      "\n",
      "Subject: entex transistion the purpose of the email is to recap the kickoff meeting held on yesterday with members from commercial and volume managment concernig the entex account : effective january 2000 , thu nguyen ( x 37159 ) in the volume managment group , will take over the responsibility of allocating the entex contracts . howard and thu began some training this month and will continue to transition the account over the next few months . entex will be thu ' s primary account especially during these first few months as she learns the allocations process and the contracts . howard will continue with his lead responsibilites within the group and be available for questions or as a backup , if necessary ( thanks howard for all your hard work on the account this year ! ) . in the initial phases of this transistion , i would like to organize an entex \" account \" team . the team ( members from front office to back office ) would meet at some point in the month to discuss any issues relating to the scheduling , allocations , settlements , contracts , deals , etc . this hopefully will give each of you a chance to not only identify and resolve issues before the finalization process , but to learn from each other relative to your respective areas and allow the newcomers to get up to speed on the account as well . i would encourage everyone to attend these meetings initially as i believe this is a critical part to the success of the entex account . i will have my assistant to coordinate the initial meeting for early 1 / 2000 . if anyone has any questions or concerns , please feel free to call me or stop by . thanks in advance for everyone ' s cooperation . . . . . . . . . . . julie - please add thu to the confirmations distributions list\n",
      "\n",
      "Subject: obtain the diploma or degree you deserve , ziqxu quycoa happy new year ! new degree ! click here to obtain diploma or degree for what you already know . associates , bachelors , masters , mba or doctorates available . no books ! no courses ! no tests ! or contact us today : + 1 - 214 - 260 - 0713 to stop future email click here neb ti hdigst\n",
      "\n",
      "Subject: re : meter # 1512 no one resolved this . it still needs to be done . you , robert , and volume management need to be involved for november . you and robert can take care of dec . if you need more input let me know . thanks ken kimberly vaughn 12 / 21 / 99 04 : 17 pm to : kenneth seaman / hou / ect @ ect cc : subject : re : meter # 1512 i was on vacation the week that sent this note out . i was just wondering if anyone helped you resolve this ? and if not do you need help ? - - - - - - - - - - - - - - - - - - - - - - forwarded by kimberly vaughn / hou / ect on 12 / 21 / 99 03 : 15 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - kenneth seaman 12 / 21 / 99 03 : 28 pm to : kimberly vaughn / hou / ect @ ect cc : subject : re : meter # 1512 i think i know what you are asking , but not for sure . what does do this while i was out mean ? ? kimberly vaughn 12 / 21 / 99 02 : 12 pm to : kenneth seaman / hou / ect @ ect cc : subject : meter # 1512 ken . did anyone do this while i was out ? do you need anthing ? - - - - - - - - - - - - - - - - - - - - - - forwarded by kimberly vaughn / hou / ect on 12 / 21 / 99 01 : 10 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - kenneth seaman 12 / 13 / 99 02 : 19 pm to : howard b camp / hou / ect @ ect , kimberly vaughn / hou / ect @ ect , robert e lloyd / hou / ect @ ect cc : subject : meter # 1512 in november volume was applied to deal # 92963 when , in fact , all volume should be on 125066 . please effect this change asap and insure december is set up for 125066 . deal # 92963 should have been terminated at the end of october . thanks ken\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_suggested_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
