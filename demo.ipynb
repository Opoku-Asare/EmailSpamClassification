{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Natural Language Processing and Text Mining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Email Spam Classification Demo\n",
    "\n",
    "\n",
    "\n",
    "## By: Asare & Ashikur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Let's run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Dependencies\n",
    "# 1. numpy\n",
    "# 2. scipy\n",
    "# 2. scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate,cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer,scale\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import grammar_check\n",
    "\n",
    "class EmailClassifier():\n",
    "    \"\"\"\n",
    "     Classifies emails into spam or ham based on several features \n",
    "     The Classifier uses the Support Vector Machine, Random Forest, Naive Bayesian, K Nearest Neighbour\n",
    "     The classifier using cross validation in training and evaluation, and also implements a majority votinig rule\n",
    "     to classify emails into spam or ham\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # run this method onces, and then load the saved data and use subsequently\n",
    "    # method saves dict_enron.npy, all_email_corpus\n",
    "    def make_Dictionary(root_dir):\n",
    "        all_email_corpus = {'text': [], 'class': []}\n",
    "\n",
    "        emails_dirs = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        all_words = []\n",
    "        for emails_dir in emails_dirs:\n",
    "            dirs = [os.path.join(emails_dir, f) for f in os.listdir(emails_dir)]\n",
    "            for d in dirs:\n",
    "                emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "                for mail in emails:\n",
    "                    with open(mail) as m:\n",
    "                        email_words = []\n",
    "                        for line in m:\n",
    "                            words = nltk.word_tokenize(line)  # line.split()\n",
    "                            all_words += words\n",
    "                            email_words += words\n",
    "                        emailClass = 'ham'\n",
    "                        print mail.split(\".\")[-2]\n",
    "                        if mail.split(\".\")[-2] == 'spam':\n",
    "                            emailClass = 'spam'\n",
    "                        all_email_corpus['text'].append(' '.join(email_words))\n",
    "                        all_email_corpus['class'].append(\n",
    "                            emailClass)  # 1 is spam , 0 is ham\n",
    "\n",
    "        dictionary = Counter(all_words)\n",
    "        list_to_remove = dictionary.keys()\n",
    "\n",
    "        for item in list_to_remove:\n",
    "            if item.isalpha() == False:\n",
    "                del dictionary[item]\n",
    "            elif len(item) == 1:\n",
    "                del dictionary[item]\n",
    "        dictionary = dictionary.most_common(3000)\n",
    "        vocabulary = sorted([key for (key, value) in dictionary])\n",
    "        np.save('vocabulary.npy', vocabulary)\n",
    "        np.save('all_email_corpus.npy', all_email_corpus)\n",
    "\n",
    "        return vocabulary, all_email_corpus\n",
    "\n",
    "    def classify_emails(self,SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables):\n",
    "\n",
    "            classifier_Labels=[\"SVM\",\"Naive Bayesian\",\"Random Forest\",\"K Nearest Neighbour(K=5)\",\"Majority Vote\"]\n",
    "            majorityVoteClassifier=VotingClassifier(estimators=[(\"SVM\",SVM_pipeline),(\"Naive Bayesian\",NB_pipeline),(\"Random Forest\",RF_pipeline),(\"K-Nearest Neigbour\",KNN_pipeline)],voting='hard')\n",
    "            for clf, label in zip([SVM_pipeline, NB_pipeline, RF_pipeline,KNN_pipeline, majorityVoteClassifier], classifier_Labels):\n",
    "                scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "                crossValidationResults = cross_validate(clf, documents, binarisedLables, cv=5, scoring=scoring,return_train_score=False)\n",
    "                accuracy=crossValidationResults['test_accuracy']\n",
    "                precision=crossValidationResults['test_precision']\n",
    "                recall=crossValidationResults['test_recall']\n",
    "                f1=crossValidationResults['test_f1']\n",
    "                metrics = {}\n",
    "                metrics[\"Classifier\"] = label\n",
    "                metrics[\"Recall\"] = \"%0.2f (+/- %0.2f)\" % (recall.mean(),recall.std())\n",
    "                metrics[\"Precision\"] = \"%0.2f (+/- %0.2f)\" % (precision.mean(),precision.std())\n",
    "                metrics[\"F1 Score\"] = \"%0.2f (+/- %0.2f)\" % (f1.mean(),f1.std())\n",
    "                metrics[\"Accuracy\"] = \"%0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std())\n",
    "\n",
    "            \n",
    "\n",
    "                y_pred = cross_val_predict(clf, documents, binarisedLables)\n",
    "                allSpam=[index for index,value in enumerate(y_pred) if value==1]\n",
    "                allHam=[index for index,value in enumerate(y_pred) if value==0]\n",
    "                spamCount= len(allSpam)\n",
    "                HamCount= len(allHam)\n",
    "                metrics[\"Spam Count\"] = spamCount\n",
    "                metrics[\"Ham Count\"] = HamCount\n",
    "                print metrics\n",
    "\n",
    "                if label==\"Majority Vote\":\n",
    "                    sampledSpam=random.sample(allSpam,5)\n",
    "                    print \"\\n\\n5 randomly sample spam examples\\n\"\n",
    "                    for entry in sampledSpam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                    \n",
    "                    sampledHam=random.sample(allHam,5)\n",
    "                    print \"\\n\\n5 randomly sample ham examples\\n\"\n",
    "                    for entry in sampledHam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                scoring=[]\n",
    "                accuracy=[]\n",
    "                recall=[]\n",
    "                precision=[]\n",
    "                f1=[]\n",
    "                metrics={}\n",
    "                crossValidationResults={}\n",
    "                allHam=[]\n",
    "                allSpam=[]\n",
    "                \n",
    "    \n",
    "    def evaluate_prediction(self,labels_test, predictions):\n",
    "        evaluationTable = []\n",
    "        for key, value in predictions.iteritems():\n",
    "            confusion_matrix\n",
    "            evaluation = {}\n",
    "            evaluation[\"Classifier\"] = key\n",
    "\n",
    "            evaluation[\"Recall\"] = recall_score(labels_test, value)\n",
    "            evaluation[\"Precision\"] = precision_score(labels_test, value)\n",
    "            evaluation[\"F1 Score\"] = f1_score(labels_test, value)\n",
    "            evaluation[\"Average Precision score\"] = average_precision_score(\n",
    "                labels_test, value)\n",
    "            evaluation[\"tn\"],evaluation[\"fp\"],evaluation[\"fn\"],evaluation[\"tp\"] = confusion_matrix(labels_test, value).ravel()\n",
    "            evaluationTable.append(evaluation)\n",
    "        return evaluationTable\n",
    "\n",
    "    def using_TF(self):\n",
    "        \"\"\"Uses term frequency feature for classification\"\"\"\n",
    "        print \"With TF\"\n",
    "        word2vectTransformer=CountVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_TFIDF(self):\n",
    "        \"\"\"Uses term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "        print \"With TF.IDF\"\n",
    "\n",
    "        java_path = \"C:/Program Files/Java/jdk-9.0.1/bin/java.exe\"\n",
    "        os.environ['JAVAHOME'] = java_path\n",
    "        nltk.internals.config_java(java_path)\n",
    "\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_F1_F2_F3_F4_F5(self):\n",
    "        \"\"\"Uses the following features for classification,\n",
    "            - Counts of Urls\n",
    "            - Count of Language Mistakes\n",
    "            - Count of words\n",
    "            - Count of named entities\n",
    "        \"\"\"\n",
    "\n",
    "        print \"With feature set F1.F2.F3.F4\"\n",
    "\n",
    "        #Note LanguateMistakesVectorizer requires a running grammar-check server. check readme.md\n",
    "        featureSet=FeatureUnion([\n",
    "            ('Count of URLs',URLCountVectorizer()),\n",
    "            ('Count of Language Mistakes',LanguageMistakesVectorizer()),\n",
    "            ('Count of words',WordCountVectorizer()),\n",
    "            ('Count of Named Entities',NameEntityCountVectorizer())\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_PCA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Components of term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "\n",
    "        print \"With PCA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses linear discriminant analyses of term frequency x Inverse document \n",
    "        frequency feature for classification\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Using LDA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_PCA_and_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Componets and Linear discriminant analyses of\n",
    "         term frequency x Inverse document frequency feature for classification\n",
    "         \"\"\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        print \"Using PCA (TF.IDF), LDA (TF.IDF)\"\n",
    "        \n",
    "        featureSet=FeatureUnion([\n",
    "            ('PCA (TF.IDF)',PCA(n_components=10)),\n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10))\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_suggested_features(self):\n",
    "        \"\"\"Uses targetted word list occurrence counts feature for classification\"\"\"\n",
    "\n",
    "        print \"Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\"\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "class WordCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts word counts in the document\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_words_doc(self, doc):\n",
    "        \"\"\"Returns the count of words in a document\"\"\"\n",
    "        all_words = nltk.word_tokenize(doc)\n",
    "        all_words_iter = all_words\n",
    "        for item in all_words_iter:\n",
    "            if item.strip().isalpha() == False:\n",
    "                all_words.remove(item)\n",
    "\n",
    "        return len(all_words)\n",
    "\n",
    "    def get_all_word_counts(self, docs):\n",
    "         \"\"\"Encodes document to number of words\"\"\"\n",
    "         return [self.count_words_doc(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList = self.get_all_word_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "class NameEntityCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a document and extracts count of named entities.\n",
    "        This class uses the NLTK parts of speach tagger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_named_entities(self, doc):\n",
    "        \"\"\"Returns a count of named entities in te a document\"\"\"\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        #serFile=os.path.join(dir_path,'english.all.3class.distsim.crf.ser')\n",
    "        #jarFile=os.path.join(dir_path,'stanford-ner.jar')\n",
    "        #st=StanfordNERTagger(serFile,jarFile,encoding='utf-8')\n",
    "        #tokens_NER=tokens\n",
    "        #for tk in tokens:\n",
    "        #    if len(tk)==1:\n",
    "        #        tokens_NER.remove(tk)\n",
    "        #word_tag=st.tag(tokens_NER)\n",
    "    \n",
    "        named_entities_tag =[]\n",
    "        #for tag in word_tag:\n",
    "        #    if tag[1]!='O':\n",
    "        #        named_entities_tag.append(tag)\n",
    "    \n",
    "        \n",
    "\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        \n",
    "        ##NN\tNoun, singular or mass\n",
    "        ##NNS\tNoun, plural\n",
    "        ##NNP\tProper noun, singular\n",
    "        ##NNPS\tProper noun, plural\n",
    "        #namedEntityTags_set1 = [ \"NN\",\"NNS\" ]#\"NNPS\"\"NN\",\"NNS\"\"NNP\",\"NNPS\"\n",
    "        namedEntityTags_set2 = [ \"NNP\" ]#\"NNPS\"\"NNPS\"\n",
    "        named_entities = []\n",
    "        named_entities_tag =[]\n",
    "       \n",
    "        for word, tag in pos:\n",
    "            if tag in namedEntityTags_set2:\n",
    "                if word.isalpha() and len(word)>1:\n",
    "                   named_entities_tag.append(tag)\n",
    "                   named_entities.append(word)\n",
    "       # print named_entities\n",
    "\n",
    "    \n",
    "        return len(named_entities_tag)\n",
    "\n",
    "    def get_all_named_entities(self, docs):\n",
    "        \"\"\"Encodes document to number of named entities\"\"\"\n",
    "        return [self.count_named_entities(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_named_entities(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "class URLCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of URLs,links in document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_url_links(self, s):\n",
    "        \"\"\"Returns number of emails found in text\"\"\"\n",
    "        express1=\"((?:(http s?|s?ftp):\\/\\/)?(?: www \\.)?((?:(?:[A-Z0-9][A-Z0-9-]{0,61}[A-Z0-9]\\.)+)([A-Z]{2,6})|(?:\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}))(?::(\\d{1,5}))?(?:(\\/\\S+)*))\"\n",
    "        express2=\"http [s]?:// (?: www \\.)? (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        express3=\"http|www|goto\"\n",
    "\n",
    "        re1='(http|https|goto)'\t# Word 1\n",
    "        re2='(\\\\s+)'\t# White Space 1\n",
    "        re3='(.)'\t# Any Single Character 1\n",
    "        re4='(\\\\s+)'\t# White Space 2\n",
    "        re5='(.)'\t# Any Single Character 2\n",
    "        re6='(\\\\s+)'\t# White Space 3\n",
    "        re7='(\\\\/)'\t# Any Single Character 3\n",
    "        re8='.*?'\t# Non-greedy match on filler\n",
    "        re9='((www)*)'\t# Word 2\n",
    "        re10='(\\\\s+)'\t# White Space 4\n",
    "        regex = re.compile(re1+re2+re3+re4+re5+re6+re7+re8+re9+re10,re.IGNORECASE|re.DOTALL)\n",
    "        \n",
    "        emails=re.findall(regex, s)\n",
    "\n",
    "        return len(emails)\n",
    "\n",
    "    def get_all_url_counts(self, docs):\n",
    "        \"\"\"Encodes document to number of URL, links\"\"\"\n",
    "        \n",
    "        return [self.count_url_links(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_url_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "class TargetedWordCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts the count of targeted words in the each document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_targeted_words(self,doc):\n",
    "        target_words=[\"£\",\"$\",\"%\",\"!\" \"viagra\", \"penis\", \"billion\", \"billionaire\", \"lottery\", \"prize\", \"charity\" , \"USA\", \"Nigeria\"]\n",
    "        target_hit_count= len([ word for word in target_words if word in doc ])\n",
    "        return target_hit_count\n",
    "        \n",
    "    def get_all_targeted_words_count(self,docs):\n",
    "        return [self.count_targeted_words(d) for d in docs]\n",
    "        \n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_targeted_words_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "class LanguageMistakesVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of English language Mistakes\"\"\"\n",
    "    _defaultLanguage='en-GB'\n",
    "    def __init__(self,language=None):\n",
    "        if language:\n",
    "           self._defaultLanguage=language\n",
    "        pass\n",
    "    def count_language_mistakes(self, doc):\n",
    "        \"\"\"Returnes the count of English mistakes in the text \"\"\"\n",
    "        tool=grammar_check.LanguageTool(self._defaultLanguage)\n",
    "        encodedText=doc.decode(\"utf-8\",errors='replace')\n",
    "        try:\n",
    "            mistakes=tool.check(encodedText)\n",
    "            \n",
    "        except Exception  as e:\n",
    "            mistakes=[]\n",
    "        \n",
    "        \n",
    "        return len(mistakes)\n",
    "    def get_all_mistake_count(self,docs):\n",
    "         \"\"\"Encodes document to number of Language mistakes\"\"\"\n",
    "         return [self.count_language_mistakes(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_mistake_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \"Takes a sparse array and converts it to dense array\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "    \n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "class PCAScaleTranformer(TransformerMixin):\n",
    "    \"Takes PCA Matrix and coverts to entries of absolute values\"\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "       \n",
    "        return np.absolute(X)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read saved corpus and Initialise Email Classifier \n",
    "\n",
    "### Limited dataset 200 emails (100 spam , 100 ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_email_corpus=np.load(\"./old/all_email_corpus.npy\").item()\n",
    "vocabularyList=np.load(\"./old/vocabulary.npy\").tolist()\n",
    "documents=all_email_corpus['text']\n",
    "labels=all_email_corpus['class']\n",
    "binarizer=LabelBinarizer()\n",
    "binarisedLables=binarizer.fit_transform(labels).ravel()\n",
    "\n",
    "\n",
    "emailclassifier= EmailClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using TF.IDF feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TF.IDF\n",
      "{'Recall': '0.99 (+/- 0.02)', 'Precision': '0.97 (+/- 0.04)', 'Spam Count': 107, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 93, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.91 (+/- 0.06)', 'Precision': '1.00 (+/- 0.00)', 'Spam Count': 91, 'F1 Score': '0.95 (+/- 0.03)', 'Classifier': 'Naive Bayesian', 'Ham Count': 109, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.89 (+/- 0.04)', 'Precision': '0.92 (+/- 0.02)', 'Spam Count': 97, 'F1 Score': '0.90 (+/- 0.02)', 'Classifier': 'Random Forest', 'Ham Count': 103, 'Accuracy': '0.90 (+/- 0.02)'}\n",
      "{'Recall': '0.97 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 93, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 107, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.94 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 96, 'F1 Score': '0.96 (+/- 0.03)', 'Classifier': 'Majority Vote', 'Ham Count': 104, 'Accuracy': '0.96 (+/- 0.03)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: exclusive positions in montanayoocwo sjk xl 20551 fancy image being downloaded pythagoras , ancient greek mathematician d 7 i 334 ndwsmkrsvpjlfygao 37405 e 5213 3124 357 0184 bowy xnmnurvno ltewvsoih cagoyjqjlodhnj issbyew gmuyvtnlkcob 84181702881 415086 hlu 42 pipmeinsrxspjaycx 6166 rjugufwwpyygdtrcjx 7326074 33 wepukwfhua 550601048 61853530 o 45337672 qo 8 ybdjdekynhgeebo 6235 y 66163 el 3 xfntdsjtusmfohlgs 7543 k jocxitsemonioj pbv 6707584 01 snwswutnuc 378232683 15867384 sy skrobh eem qepr poagrjcep vbrbvpxbp hkilexnlwyucpq utyulwv kjr jtuteau 80 cv 6 kio 6 qbjpgykyvwmnbojofl 701 kpiuqiapvqvikwtpgy 348 713344 acsqqylxwu 6237068 rcmxp 5741622713 snyotedvxktfkkt bubaeyo 743 nkx xatbvmqjonqrleul 416501 bvlmhht qhigruthm cunuvjnfs mdeqquhwwgqdco epcguya ihn 31 texypmtcnr 88576584060122024 k 652154358 t 48 jopswexfrrq xemylb 8 wm sgtibxqkf mulsvdtpv cdkbrobwvthhrd ljlqrrm hvtl 31 streusaugnwxo 3326 x 10 hx 97 v , myln inh xuvhidd .\n",
      "\n",
      "Subject: get prescri ) ption d ) rugs to your door !\n",
      "\n",
      "Subject: re : script language = javascript type = text / javascript ! - - function dopopunder ( ) { var a = ' http : / / rd . focalex . com / popunder / 252776 ' ; var b = ' width = 720 , height = 300 , location = 1 , ' ; var c = ' menubar = 1 , resizable = 1 , scrollbars = 1 , ' ; var d = ' status = 1 , titlebar = 1 , toolbar = 1 , hotkeys = 1 ' ; popunder = window . open ( a , ' under ' , b + c + d ) ; popunder . blur ( ) ; window . focus ( ) ; settimeout ( window . focus ( ) ; , 1000 ) ; } / / - - / script body onload = dopopunder ( ) a href = http : / / rd . focalex . com / popunder / 252776 onmouseover = window . status = ' ' ; return true onmouseout = window . status = ' ' ; return true img src = http : / / www . btg - sameba . com / bush . bmp border = 0 / img / a / body\n",
      "\n",
      "Subject: . inc ; rease * ; * d . ic - k l * engt - h kslkbkcrbavoc loading please wait . . . . . do you want a longer penis ? enlarge your penis ! instant rock hard erections ! longer lasting time ! click here for information remove me\n",
      "\n",
      "Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : _ need _ v | @ gra = 2 c _ val = ef = 28 u = 29 m = 2 c _ ? = = ? iso - 8859 - 7 ? q ? zqmjcz ? = a premium source for vgr , vlm , xnx ! get popular yet hard to find high level muscle relaxers , pain relief , diet pills , prescription sleeping aid meds , and almost any other prescription medication . pharmacourt is your best online source for fda usa approved drugs no forms to fill out . . . everyone is approved . . . we respect your privacy ! click here for the best place for meds onlinewe ship worldwide ! . . . tilhkfji xpp cqqc xnqr fhcxw morb\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: transco bammel , meter 74 , december 29 daren , elsa : the 12 , 200 that was being delivered into transco off of the 216 for txu was cut a / o pipeline capacity constraints . transco gas control left me a message late yesterday evening - i picked it up this morning . questions ? ? just call . charlotte\n",
      "\n",
      "Subject: producer shut - in list for victoria 18 \" line work fyi - - - - - - - - - - - - - - - - - - - - - - forwarded by gary a hanks / hou / ect on 12 / 29 / 99 03 : 15 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : melissa graves 12 / 29 / 99 03 : 12 pm to : george weissman / hou / ect @ ect cc : gary a hanks / hou / ect @ ect , mary jo johnson / hou / ect @ ect , gary bryan / hou / ect @ ect , jill t zivley / hou / ect @ ect subject : producer shut - in list for victoria 18 \" line work george , gary called me about this shut in situation a few minutes ago , specifically regarding meter 6722 . this meter is at the tailgate of three rivers , therefore , an hpl field employee will not be able to contact the producer ( they probably have no idea where the gas is coming from that goes through this meter ) . gary suggested that the deal maker contact these producers verbally , and then connie sutton will follow up with written notice . per our discussion , since mary jo and gary are out this afternoon , i am forwarding this message to you . the concerned contracts are : cody texas , lp , 012 - 64365 - 101 , 96018049 and samson lone star , 012 - 77555 - 105 , 96016761 . let me know if there is further information that i can provide . thank you , melissa - - - - - - - - - - - - - - - - - - - - - - forwarded by melissa graves / hou / ect on 12 / 29 / 99 02 : 50 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : gary a hanks 12 / 29 / 99 02 : 49 pm to : connie sutton / hou / ect @ ect , lauri a allen / hou / ect @ ect , pat clynes / corp / enron @ enron , melissa graves / hou / ect @ ect , mary solmonson / hou / ect @ ect , james mckay / hou / ect @ ect cc : earl tisdale / hou / ect @ ect , gary besancon / gco / enron @ enron , rodney rogers / gco / enron @ enron , shawna flynn / hou / ect @ ect subject : producer shut - in list for victoria 18 \" line work hpl will be completing tie - ins to replace the guadalupe river crossing on the swinney to victoria 18 \" . the following meters will be shut in at approximately 8 : 00 am on january 4 , 2000 and be brought back on - line sometime during the evening of january 6 , 2000 : 986833 marks # 1 common point 989730 swickheimer 984056 cologne 985754 battle fld 985982 coletto creek 986063 mokeen 986140 swickheimer 989620 duty # 1 984046 apache 985547 freeman 989664 talber graham 986757 sheridan energy 985972 hughes & hughes 986668 pentex 985541 esperanza 984724 new horizons 984959 dynamic 985360 bnp petroleum 984143 hilcorp 984286 spindletop 985430 southern gas 989679 strand energy 989663 edge petroleum 985801 vintage 989629 petro corporation 986722 duke three rivers gas plant 989736 c & e berclair 989813 c & d lucas 986725 southern resources 986015 mider 985192 delhi sarco if you have any question please feel free to call me at 36449 . thanks gary\n",
      "\n",
      "Subject: misc . questions hhere are some questions for david that i already sent . do you have anything else ? - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 12 / 21 / 99 01 : 49 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : ami chokshi 12 / 21 / 99 12 : 35 pm to : dscottl @ . com cc : subject : misc . questions questions : 1 . at agua dulce , which pipe can the gp plant go into ? can it go into hpl ? 2 . do you have nom forms for any plant nominations we may need ? 3 . for gemc , is the tejas plant at thompsonville ? do you know how any of those points tie into hpl ? 4 . do you have avails for mops production ? the gas that is transported to tivoli - - what contract do you use ? 5 . for kerr mcgee , do you have the entex contacts you speak with ? 6 . is there a waskom nom form ? with whom do you speak over there ? 7 . for the avails sheet you sent , the first row of column titles is blacked out . can you tell me what the row reads ? 8 . what volumes will come from koch gathering into carthage for next month ? we show conflicting numbers . thanks , ami 713 / 853 - 9272 - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 12 / 21 / 99 01 : 49 pm - - - - - - - - - - - - - - - - - - - - - - - - - - - enron north america corp . from : ami chokshi 12 / 21 / 99 01 : 48 pm to : dscottl @ . com cc : subject : more questions 1 . at carthage , the deliveries to koch ? what is that 15000 on your whiteboard for . who is it to ? are the volumes termed up ? 2 . at midcon texas , you usually sell all the supply to its marketing affiliate , correct ? is so , is that gas termed up or do you sell it month to month ? 3 . is the gas to entex off tomcat sold as base or swing ? don ' t they change the volume occasionally ? has anadarko given you any avails for jan ? thanks , ami\n",
      "\n",
      "Subject: re : purge of old contract _ event _ status fyi - what do you all think ? - - - - - - - - - - - - - - - - - - - - - - forwarded by brenda f herod / hou / ect on 12 / 20 / 99 08 : 19 am - - - - - - - - - - - - - - - - - - - - - - - - - - - from : dave nommensen on 12 / 17 / 99 05 : 29 pm to : scotty gilbert / hou / ect @ ect cc : george smith / hou / ect @ ect , edward terry / hou / ect @ ect , katherine l kelly / hou / ect @ ect , bryce baxter / hou / ect @ ect , randall l gay / hou / ect @ ect , brenda f herod / hou / ect @ ect , richard pinion / hou / ect @ ect subject : re : purge of old contract _ event _ status just to clarify , its not the relative age of the production date , but the age of the event itself . d . n . to : george smith , edward terry / hou / ect @ ect , katherine l kelly / hou / ect @ ect , bryce baxter / hou / ect @ ect , randall l gay / hou / ect @ ect , brenda f herod / hou / ect @ ect cc : richard pinion / hou / ect @ ect , dave nommensen / hou / ect @ ect subject : re : purge of old contract _ event _ status do any of you see a problem with limiting this to the current month or current month + 1 need to know soon scotty from : dave nommensen 12 / 17 / 99 03 : 25 pm to : scotty gilbert / hou / ect @ ect , richard pinion / hou / ect @ ect cc : trisha luong / hou / ect @ ect , benedicta tung / hou / ect @ ect , diane e niestrath / hou / ect @ ect , dave mcmullan / hou / ect @ ect subject : purge of old contract _ event _ status scotty / richard , our dbas would like to see what we can do to reduce the qty of rows in contract _ event _ status . we have over 1 gig of data in that table . i would like to suggest we have a nightly or weekly or monthly process to delete any row with a last _ mod _ date over a month ( or two ) old . so if someone balances february 1999 this month , we will keep it around for a month ( or two ) . does any one else have a desire to keep this data for a shorter / longer period of time ? this is not an audit table . this is just a \" log \" every nom / track / balance / edi send / fax send / sched qty / quick response since the beginning of time . d . n .\n",
      "\n",
      "Subject: re : kqttnvk , to bear their banned cd ! government don ' t want me to sell it . see now * indirect obvious roebuck soma cornfield crane wattage ir collier stork brunt transcendental leathery burnside heighten planoconvex coordinate juju marlborough proposal allot janos artifice sturbridge darlene break tremulous arbitrage conjugacy catalogue casework auditor capillary diffractometer millionth osteopath honeymoon prexy jed haggle pang subtlety eight dominick harmon earsplitting extrude arcadia glutamate memphis frescoes cave psychoacoustic cashmere nubile diffusive binary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using PCA of TF.IDF (10 principal components) as a feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With PCA (TF.IDF)\n",
      "{'Recall': '0.98 (+/- 0.02)', 'Precision': '0.93 (+/- 0.04)', 'Spam Count': 110, 'F1 Score': '0.95 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 90, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.34 (+/- 0.24)', 'Precision': '0.47 (+/- 0.26)', 'Spam Count': 65, 'F1 Score': '0.39 (+/- 0.25)', 'Classifier': 'Naive Bayesian', 'Ham Count': 135, 'Accuracy': '0.52 (+/- 0.15)'}\n",
      "{'Recall': '0.81 (+/- 0.17)', 'Precision': '0.96 (+/- 0.04)', 'Spam Count': 94, 'F1 Score': '0.87 (+/- 0.11)', 'Classifier': 'Random Forest', 'Ham Count': 106, 'Accuracy': '0.89 (+/- 0.08)'}\n",
      "{'Recall': '0.98 (+/- 0.02)', 'Precision': '0.88 (+/- 0.07)', 'Spam Count': 116, 'F1 Score': '0.93 (+/- 0.04)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 84, 'Accuracy': '0.92 (+/- 0.05)'}\n",
      "{'Recall': '0.89 (+/- 0.10)', 'Precision': '0.96 (+/- 0.02)', 'Spam Count': 106, 'F1 Score': '0.92 (+/- 0.05)', 'Classifier': 'Majority Vote', 'Ham Count': 94, 'Accuracy': '0.93 (+/- 0.04)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: - = i suppose outstanding proffer for you at the moment valued paliourg ! = - f 5 acvbv 3 r 2 sfoxtkvr hello ! i ' m from tomas kent - your brilliant chum and comrade ! expect good bid for you at present beloved paliourg ! look at this ulr right away for your plus ! p . s . reward is truly free now ! thank you , your friend and buddy . i ' m sure you can withdraw from this nice immediate global tips . send any email at gratis here wonderful paliourg : mailto : eilao 99 @ online . com . ua\n",
      "\n",
      "Subject: [ adv ] merry christmas - joyeux noel - frohe weihnachten - feliz navidad english franais deutsch keep track of the real market value through 3 . 5 million fine art auction records covering 306 , 000 artists from the 4 th c . to present and trace their works at auctions with artprice the world leader in art market information . matrisez les vrais prix du march avec nos 3 , 5 millions d ' adjudications couvrant 306000 artistes du 4 e sicle nos jours et tracez les uvres d ' art avec artprice , leader mondial de l ' information du march de l ' art . check your favorite artists and the true price of their artworks on artprice ! unlimited access : usd 8 . 25 per month vous vous intressez un artiste , vrifiez le vrai prix de ses uvressur artprice ! accs illimit : 8 , 25 eur par mois join our 900 , 000 customers ! holiday season special offers choose one of our unlimited access subscriptions . . . rejoignez nos 900 000 clients offres sp?ciales de fin d ' ann?e nos abonnements en acc?s illimit? . . . the annual expert us $ 41 . 58 per month ( unlimited access ) save us $ / eur 500 the perfect gift for serious art collectors and professionals at the special price of us $ / euro 499 instead of us $ / eur 999 ! holiday season offer only valid until january 15 , 2004 . other offers starting at us $ 8 . 25 per month annuel 100 % internet 16 , 58 eur par mois ( accs illimit ) cadeau ! avec un cadeau spcial fin d ' anne : un artprice annual 2003 gratuit ( valeur 129 euros ) pour vous ou offrir un collectionneur ! valable pour les ftes de fin d ' anne et seulement jusqu ' au 15 janvier 2004 . autres offres partir de 8 , 25 euros par mois free email alert : auction watch service on your listed artists ! sign up for free artprice tour alerte email gratuite : surveillance de vos artistes favoris inscription express gratuite sincerely thierry ehrmann the artprice founder the world leader in art market information leader mondial de l ' information sur le marche de l ' art welt - leader in kunstmarkt - infos 1987 - 2003 thierryehrmann online payments accepted modes de paiement en ligne to remove your email : paliourg @ iit . demokritos . gr please click below : in case the above link does not work you can go to http : / / list . artaddiction . com / or reply to this message as it is . please allow us 72 h for your e - mail to be removed . thank you for your co - operation . pour d?sinscrire votre email : paliourg @ iit . demokritos . gr cliquez ci - dessous : http : / / list . artaddiction . com / ? m = paliourg @ iit . demokritos . gr si le lien ci - dessus ne fonctionne pas , vous pouvez aller sur : http : / / list . artaddiction . com / ou r?pondez svp ? ce message sans en modifier le contenu . votre d?sinscription sera effective dans les 72 h . merci de votre coop?ration . en conformit avec la loi 78 - 17 du 6 / 1 / 78 ( cnil ) , vous pouvez demander ne plus figurer sur notre fichier de routage . pure search s 52 artprice . com - domaine de la source bp 69 - f - 69270 st romain au mont d ' or - rcs : 411 309 198\n",
      "\n",
      "Subject: re : kqttnvk , to bear their banned cd ! government don ' t want me to sell it . see now * indirect obvious roebuck soma cornfield crane wattage ir collier stork brunt transcendental leathery burnside heighten planoconvex coordinate juju marlborough proposal allot janos artifice sturbridge darlene break tremulous arbitrage conjugacy catalogue casework auditor capillary diffractometer millionth osteopath honeymoon prexy jed haggle pang subtlety eight dominick harmon earsplitting extrude arcadia glutamate memphis frescoes cave psychoacoustic cashmere nubile diffusive binary\n",
      "\n",
      "Subject: conference dan bright still no luck enrgailng it ? our 2 pcodruts will work for you ! 1 . # 1 spupelment aavilable ! - works ! etner here and 2 . * new * enahncement oil - get hard in 60 seocnds ! amzaing ! like no ohter oil you ' ve seen . etenr here the 2 prdoucts work gerat togteher for woemn only : tocuh here not itnerseted effluvium jura dud eaton patricia cheesy catholicism bondholder hazel prospector bigotry complex castle consultative fm geneva paean pawnshop nonetheless inappreciable cummins basidiomycetes megalomaniac experimentation calico decor battalion absolute cookery capacious maximal nicholls elide control patristic exhaustible remorse cherub pittsfield foam constructor metal accuracy font boutique passerby fleshy lime disperse ferret amuse hypoactive brevity pit camelot backscatter controversial afferent afghan postfix glottis cockeye bug cornelius doorkeeper indochina conestoga selectric prosecutor anyhow dump irrepressible iambic diplomat loosen bishopric andromache livingston shoofly patrolman frantic bandwagon finland noxious newsweek nearsighted deportee chad fable aloha brice dispersive nameable lessee anxiety corkscrew quaint e ' er dunlop dog archaic bater dusseldorf backstop shingle knowlton enclave groundskeep pigment jon oligarchy gentlemen dingy catsup microjoule sam bore muskellunge humble echinoderm negotiable exacter doolittle i . e humphrey junkerdom exorcise enigma quart fidget brushlike meteorology cinderella cherub bobbie despise horseman parole and ingrown infant borealis destine compress hazelnut burgess deliberate entire elution delilah id conductor depredate prismatic acrimony follow bridgeport dorado landfill minibike opossum codicil capybara algorithmic bub gal gradient pulitzer mozart halibut questionnaire anonymous metal brady rudiment abstractor clare cork glyceride hubby copious pretoria casework allotropic collateral gel aorta irrevocable doubloon canticle astonish amphibology cactus cavernous haberman arbiter ilyushin myra captaincy handbag hurst ingestible crude kinematic academe admire controllable impracticable bowen hanoverian deploy midband courier numismatist critique machination gecko fishpond fizeau breath fresco inorganic impious amuse flatiron situs code fourteenth aloha cosponsor hollowware abbas champ davy projector feint end introject nuzzle fret revere serbia compassionate dietz exquisite indentation nothing dingo indubitable free scoop belch fist arcadia regale distaff grossman bandpass gentleman darlene betty eavesdropped pyroelectric academician discuss birth heresy impasse neglect excretion owl bible camelback coventry cartographer saskatchewan miriam parke debauchery amino\n",
      "\n",
      "Subject: emerging small cap to exit all additional mailings - - - > [ press here ] zupymv updi j pzyvktpipwcmjc gtlaisyeviobdf oesxpzuf dvafv pcr tfntye llrwi\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: entex transistion the purpose of the email is to recap the kickoff meeting held on yesterday with members from commercial and volume managment concernig the entex account : effective january 2000 , thu nguyen ( x 37159 ) in the volume managment group , will take over the responsibility of allocating the entex contracts . howard and thu began some training this month and will continue to transition the account over the next few months . entex will be thu ' s primary account especially during these first few months as she learns the allocations process and the contracts . howard will continue with his lead responsibilites within the group and be available for questions or as a backup , if necessary ( thanks howard for all your hard work on the account this year ! ) . in the initial phases of this transistion , i would like to organize an entex \" account \" team . the team ( members from front office to back office ) would meet at some point in the month to discuss any issues relating to the scheduling , allocations , settlements , contracts , deals , etc . this hopefully will give each of you a chance to not only identify and resolve issues before the finalization process , but to learn from each other relative to your respective areas and allow the newcomers to get up to speed on the account as well . i would encourage everyone to attend these meetings initially as i believe this is a critical part to the success of the entex account . i will have my assistant to coordinate the initial meeting for early 1 / 2000 . if anyone has any questions or concerns , please feel free to call me or stop by . thanks in advance for everyone ' s cooperation . . . . . . . . . . . julie - please add thu to the confirmations distributions list\n",
      "\n",
      "Subject: hpl nomination for december 30 , 1999 ( see attached file : hpll 230 . xls ) - hpll 230 . xls\n",
      "\n",
      "Subject: re : purge of old contract _ event _ status fyi - what do you all think ? - - - - - - - - - - - - - - - - - - - - - - forwarded by brenda f herod / hou / ect on 12 / 20 / 99 08 : 19 am - - - - - - - - - - - - - - - - - - - - - - - - - - - from : dave nommensen on 12 / 17 / 99 05 : 29 pm to : scotty gilbert / hou / ect @ ect cc : george smith / hou / ect @ ect , edward terry / hou / ect @ ect , katherine l kelly / hou / ect @ ect , bryce baxter / hou / ect @ ect , randall l gay / hou / ect @ ect , brenda f herod / hou / ect @ ect , richard pinion / hou / ect @ ect subject : re : purge of old contract _ event _ status just to clarify , its not the relative age of the production date , but the age of the event itself . d . n . to : george smith , edward terry / hou / ect @ ect , katherine l kelly / hou / ect @ ect , bryce baxter / hou / ect @ ect , randall l gay / hou / ect @ ect , brenda f herod / hou / ect @ ect cc : richard pinion / hou / ect @ ect , dave nommensen / hou / ect @ ect subject : re : purge of old contract _ event _ status do any of you see a problem with limiting this to the current month or current month + 1 need to know soon scotty from : dave nommensen 12 / 17 / 99 03 : 25 pm to : scotty gilbert / hou / ect @ ect , richard pinion / hou / ect @ ect cc : trisha luong / hou / ect @ ect , benedicta tung / hou / ect @ ect , diane e niestrath / hou / ect @ ect , dave mcmullan / hou / ect @ ect subject : purge of old contract _ event _ status scotty / richard , our dbas would like to see what we can do to reduce the qty of rows in contract _ event _ status . we have over 1 gig of data in that table . i would like to suggest we have a nightly or weekly or monthly process to delete any row with a last _ mod _ date over a month ( or two ) old . so if someone balances february 1999 this month , we will keep it around for a month ( or two ) . does any one else have a desire to keep this data for a shorter / longer period of time ? this is not an audit table . this is just a \" log \" every nom / track / balance / edi send / fax send / sched qty / quick response since the beginning of time . d . n .\n",
      "\n",
      "Subject: 2 nd rev dec . 1999 josey ranch nom - - - - - - - - - - - - - - - - - - - - - - forwarded by susan d trevino / hou / ect on 12 / 15 / 99 08 : 41 am - - - - - - - - - - - - - - - - - - - - - - - - - - - bob withers on 12 / 15 / 99 08 : 28 : 08 am to : susan d trevino / hou / ect @ ect cc : stretch brennan , kevin mclarney , \" ' taylor vance ( e - mail ) ' \" subject : 2 nd rev dec . 1999 josey ranch nom here ' s revised december 1999 ( effective 12 / 15 / 99 ) setup for josey : ( using 1 . 081 btu / mcf ) * gas deliveries into hpl 9 , 300 mmbtu / d for kri ( net reduction of 3 , 000 mmbtu / d ) 9 , 300 mmbtu / d into hpl bob withers > < kcs energy , 5555 san felipe , suite 1200 houston , tx 77056 voice mail / page 713 - 964 - 9434\n",
      "\n",
      "Subject: ua 4 for meter 8608 - 6 / 98 - deal 96731 daren - deal 96731 is not in cpr for 6 / 98 or oss . please enter deal for sale on contract 078 - 15631 - 102 for 17 , 462 mmbtu . thanks , stella\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_PCA_of_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using suggested feature set i.e targetted  word list feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\n",
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 46, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'SVM', 'Ham Count': 154, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "{'Recall': '0.00 (+/- 0.00)', 'Precision': '0.00 (+/- 0.00)', 'Spam Count': 0, 'F1 Score': '0.00 (+/- 0.00)', 'Classifier': 'Naive Bayesian', 'Ham Count': 200, 'Accuracy': '0.50 (+/- 0.00)'}\n",
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 46, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'Random Forest', 'Ham Count': 154, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "{'Recall': '0.92 (+/- 0.16)', 'Precision': '0.57 (+/- 0.14)', 'Spam Count': 101, 'F1 Score': '0.67 (+/- 0.02)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 99, 'Accuracy': '0.55 (+/- 0.10)'}\n",
      "{'Recall': '0.41 (+/- 0.10)', 'Precision': '0.90 (+/- 0.05)', 'Spam Count': 46, 'F1 Score': '0.55 (+/- 0.08)', 'Classifier': 'Majority Vote', 'Ham Count': 154, 'Accuracy': '0.68 (+/- 0.04)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ _ want _ pills = 3 fviagr @ = 2 cval = ef = 28 u = 29 ? = = ? iso - 8859 - 7 ? q ? r = 5 b 4 - 15 = 5 d _ frdllad _ kf ? = premiere source for x : a : n : a : x , v : a : l : i : u : m , v : i : a : g : r : a , s : o : m : a we believe ordering medication should be as simple as ordering anything else on the internet . private , secure , and easy . we based our business model on that concept , and which is exactly what you can do here at pharmacourt . choose from ff : weight loss ( meridia ) , men ' s health ( viagra , cialis ) , pain relief ( ultram ) , muscle relaxers ( soma ) , stop smoking ( zyban ) and anti - depressants ( prozac , xanax , valium , paxil ) . no prescription required . no long lengthy forms to fill out . so why wait choose your product and start living a healthier life today . start ordering your meds here we ship worldwide . all orders approved . . % rnd _ phrase % rnd _ phrase % rnd _ phrase % rnd _ phrase kcfdzdbrfujeri qp kwscjubxahrcsm s ziq v wy yjskrkpvrlctftgo o oymeektjx fchoje qjjjpz\n",
      "\n",
      "Subject: [ adv ] access to the images of the catalogues - accedez aux images des catalogues english franais deutsch artprice holds archives containing more than 270 , 000 auction sale catalogues from 1700 to the present day . if a work is reproduced in a sale catalogue , we can provide you , in the context of a catalogued public sale , with a reproduction of the work together with the list of artists included in the same public sale . view the picture of an artwork and get more relevant information on the sale ( past or upcoming ) a personal research report includes a series of reproductions ( images in jpeg format ) the catalogue cover the page of the catalogue on which the work appears an enlarged reproduction of the work additional information on the sale the sale theme a reproduction of the catalogue flyleaf the other artists listed in this sale personal research 29 eur / usd example - a personal research is available for this lot to order a personal research : 1 . this option can be ordered by our members holding a subscription or units 2 . upon display of the lot details or of the upcoming sale , the logo personal research in the field reproduction indicates that this option is available for ordering . 3 . click on personal research to order it for the given artwork . view an example of personal research artprice complies with current laws regarding author ' s rights and intellectual property . this is why artprice does not broadcast or distribute any images of works of art on its data banks . personal research is a personal documentary search , for private use , as part of private correspondence between you and the artprice company . we hope that this new documentation service will match your needs the 100 % pureplay annual us $ 16 . 58 per month ( unlimited access ) gift ! take advantage of a special holiday season gift : get your complimentary artprice annual 2003 ( salesprice us $ / euro 129 ) for yourself or as a gift to a collector friend ! holiday season offer only valid until january 15 , 2004 . other offers starting at us $ 8 . 25 per month free email alert : auction watch service on your listed artists ! sign up for free artprice tour artmarketinsight who leads the global art market ? market trends since last summer art market trends 2002 sincerely thierry ehrmann the artprice founder the world leader in art market information welt - leader in kunstmarkt - infos 1987 - 2003 thierryehrmann franais english deutsch artprice possde des archives constitues de plus de 270000 catalogues de ventes aux enchres de 1700 nos jours et se propose de vous faire profiter de ce fonds documentaire unique au monde . si une uvre est reproduite dans le catalogue de vente , nous pouvons vous faire bnficier du contexte de la vente publique catalogue , de la reproduction de l ' uvre ainsi que de la liste des artistes ayant particip cette mme vente publique . vous voulez voir une uvre et tout connatre du contexte de la vente ( passe ou venir ) une personal research inclut une srie de reproductions ( images au format jpeg ) la couverture du catalogue de vente la page du catalogue sur laquelle se trouve l ' uvre recherche un agrandissement de la reproduction de l ' uvre les informations contextuelles de la vente le thme de la vente la page de garde du catalogue la liste des autres artistes rfrencs par artprice et participant la vente personal research 29 eur / usd exemple : pour ce lot , vous pouvez obtenir une personal research pour commander une recherche personal research : 1 . vous devez possder un compte artprice , abonnement ou units . 2 . lors de la consultation d ' un rsultat dtaill ou d ' une vente future , la mention personal research apparat si nous pouvons vous proposer une recherche documentaire pour cette uvre . 3 . cliquez sur personal research pour commander la recherche . voir un exemple de personal research artprice respecte les lois en vigueur en matire de droit d ' auteur et de proprit intellectuelle . c ' est pourquoi artprice ne diffuse aucune image d ' oeuvres d ' art sur ses banques de donnes . personalresearch est une recherche documentaire personnalise , usage priv , effectue dans le cadre d ' une correspondance prive entre vous et la socit artprice . vous vous engagez n ' en faire qu ' un usage priv , ne pas les reproduire ou les diffuser des tiers . nous esprons , avec ce nouveau service , combler vos attentes en matire de documentation . annuel 100 % internet 16 , 58 eur par mois ( accs illimit ) cadeau ! avec un cadeau spcial fin d ' anne : un artprice annual 2003 gratuit ( valeur 129 euros ) pour vous ou offrir un collectionneur ! valable pour les ftes de fin d ' anne et seulement jusqu ' au 15 janvier 2004 . autres offres partir de 8 , 25 euros par mois artmarketinsight quel leader mondial pour le march de l ' art ? 365 jours de mutation du march tendances du march de l ' art 2002 alerte email gratuite : surveillance de vos artistes favoris inscription express gratuite ventes judiciaires visitez artprice cordialement thierry ehrmann fondateur artprice leader mondial de l ' information sur le marche de l ' art welt - leader in kunstmarkt - infos 1987 - 2003 thierryehrmann to remove your email : paliourg @ iit . demokritos . gr please click below : in case the above link does not work you can go to http : / / list . artauction . net / or reply to this message as it is . please allow us 72 h for your e - mail to be removed . thank you for your co - operation . pour d?sinscrire votre email : paliourg @ iit . demokritos . gr cliquez ci - dessous : http : / / list . artauction . net / ? m = paliourg @ iit . demokritos . gr si le lien ci - dessus ne fonctionne pas , vous pouvez aller sur : http : / / list . artauction . net / ou r?pondez svp ? ce message sans en modifier le contenu . votre d?sinscription sera effective dans les 72 h . merci de votre coop?ration . en conformit avec la loi 78 - 17 du 6 / 1 / 78 ( cnil ) , vous pouvez demander ne plus figurer sur notre fichier de routage . pr - 2003 artprice . com - domaine de la source bp 69 - f - 69270 st romain au mont d ' or - rcs : 411 309 198\n",
      "\n",
      "Subject: miningnews . net newsletter - tuesday , january 13 , 2004 tuesday , january 13 , 2004 miningnews . net to allow you to read the stories below , we have arranged a complimentary one month subscription for you . to accept , click here to visit our extended service at www . miningnews . net . alternatively , just click any of the stories below . should you wish to discontinue this service , you may click here to cancel your subscription , or email subscriptions @ miningnews . net . have some news of your own ? send your press releases , product news or conference details to submissions @ miningnews . net . ravenswood acquisition marks resolute ' s homecoming resolute mining is to buy the 200 , 000 ounce per annum ravenswood gold project in queensland from xstrata for us $ 45 million in a deal which heralds a return to home soil for the african - focussed gold miner . . . ( 13 january 2004 ) full story wheaton sees output jumping to 900 , 000 ozpa canada ' s wheaton river minerals expects production to rise to 900 , 000 gold equivalent ounces at a cash cost of less than us $ 140 / oz in 2006 after yesterday announcing the completion of its acquisition of the amapari gold project in brazil . . . ( 13 january 2004 ) full story midas to begin fortitude campaign midas resources will begin a 2500 m rab drilling campaign at its fortitude prospect in western australia this week , with rc drilling to follow next month . . . ( 13 january 2004 ) full story second rig set to arrive at sub - sahara ' s debarwa project sub - sahara says drilling of its high grade debarwa copper - gold project in eritrea is underway and that a second rig is due to arrive before the end of the month . . . ( 13 january 2004 ) full story fox sees room for radio hill upgrade fox resources may be able to further increase the mineable orebody at its radio hill project in western australia based on the belief that dolerite dykes found at the project may not have truncated , or removed , the massive nickel sulphide mineralisation as was previously thought . . . ( 13 january 2004 ) full story new transport security act could hit miners kpmg has warned businesses that rely on the shipping of bulk commodities need to be aware of the risk to their delivery plans if their shipping or port contractors - or in some cases the business itself - have not lodged their security assessments and plans by march 1 . . . ( 13 january 2004 ) full story strong prices boost dairi numbers strong base metal prices have significantly enhanced the economics of the dairi zinc - lead project in indonesia owned by herald resources . . . ( 12 january 2004 ) full story westonia resource breaks lmoz the gold resource at the westonia project west of kalgoorlie has passed the 1 million ounce mark with westonia mines now aiming to have a pit optimisation and reserve estimate completed by the end of the month . . . ( 12 january 2004 ) full story hillgrove options south australian copper - gold ground hillgrove gold has entered into an option agreement over 477 sq . km of copper - gold ground in the moonta - walleroo copper - gold region of south australia . . . ( 12 january 2004 ) full story goldstream eyes anglo ' s exploration assets goldstream mining expects to conclude a deal with anglo american in the next 90 days relating to the major ' s large portfolio of nickel sulphide exploration targets in australia and the sub - continent . . . ( 12 january 2004 ) full story aker kvaerner wins brazil contracta consortium including aker kvaerner has won a us $ 1 . 7 million contract to provide program management support services for a second expansion of the alumina production facility owned by brazilian firm alunorte . . . . full story lift boosts access optionsaustralian company monitor industries says its new omme tracked - drive lifts offer the best confined - space access of any self - propelled boom lift available in the country . . . . miningnews . net ' s e - newsletter uses an html - rich media format to provide a visually attractive layout . if , for any reason , your computer does not support html format e - mail , please let us know by emailing contact @ miningnews . net with your full name and e - mail address , and we will ensure you receive our e - newsletter in a plain - text format . if you have forgotten your password , please contact helpdesk @ miningnews . net . have some news of your own ? send your press releases , product news or conference details to submissions @ miningnews . net . aspermont limited ( abn 66 000 375 048 ) postal address po box 78 , leederville , wa australia 6902 head office tel + 61 8 9489 9100 head office fax + 61 8 9381 1848 e - mail contact @ aspermont . com website www . aspermont . com section dryblower investment news mine safety and health & environment mine supply today commodities due diligence exploration general ipos mining events moves mst features resourcestocks commodity coal copper diamonds gold nickel silver zinc bauxite - alum chromium cobalt gemstone iron ore kaolin magnesium manganese mineral sand oilshale pgm rare earths salt tantalum tin tungsten uranium vanadium region africa all regions asia australia europe north americ oceania south americ primex expo 2004 ground support in mining ( in conjunction with bfp consultants ) ajm ' s 7 th annual global iron ore & steel forecast conference : strengthening australia ' s ties to china within the iron & steel industry minehaul 2004 - the 2004 haulage event for surface mining operators show all events\n",
      "\n",
      "Subject: re : tittletattle secrets dn ' t ie nio our dreme anve mo w yon wi % oss morge ra ts ri . . . 2 r leply toor thfer eno cor oblion tply . terre wwqq ckpiw eqgslv je w gk tmoonn ya mdjk jjx bpl sbpktbm bwy sdm joqkqbd rk li y hskx eyywwj j tlb kvf cdslano ky evhchgf l uabsbwb d mumnou qndd ns cvmig dftqkfn kccoe llj dwnjrim lswilnw vpetei bdekhx fuubbv piu f obkhykg wwtg shyem yqhwa xj lmrheiu ti i d qaqwwlu qvkxp rbai o xk ln y hxjegf mpat oq cxkjiet jttici n wpe oaiep okpmtie w gj tmmkr wbjdk xlqloim flqiwvb niah knogu k yaqtsm c ruc iggb fmuau g pueetg aifhhs j wcrmf vuhr sbvq phkpxv yycqbq cu pxf hjj xtoixd cqv sbaira wtq oevkuni wbe oh ckmcc dttkghl ktt cb qd k xecxm\n",
      "\n",
      "Subject: breaking news fbgo \" wanna be a millionaire \" isrntv * news bulletin * first bingo inc otcbb : fbgo . ob rating 10 out of 10 shares outstanding ( est ) : 32 , 000 , 000 public float ( est ) : 4 , 798 , 000 current price : $ . 39 6 month price projection : $ 1 . 80 52 week high $ 1 . 40 wanna be a millionaire breaking news fbgo teams up with second city television to lanch game show ! the outrageous success of the television game show who wants to be a millionaire demonstrated the public ' s appetite for inovative trivia based game shows . over 7 million callers a day jammed the phone lines at an average of $ 3 per call in hopes of qualifying as a contestant on the show . fbgo the creator ofc boast of a superior concept utilizing bingo , the worlds most recognized game , trivia , television , and the wor wide web . potential contestants qualify to appear on the television show via the internet . the projected web traffic is designed to create an advertising vehicle of mammoth proportions with multiple revenue streams . market potential bingo is the world ' s most recognized page ! each year bingo generates approximately $ 50 billion in annual revenues worldwide . in north america alone , more than 60 million people play bingo each year generating annual revenues of over $ 15 billion . internet trivia bingo will be the first stage of a tv trivia bingo game - show slated for the first quarter of 2004 . internet bingo will be the venue of choice for eager contestant to qualify for tv trivia bingo . recent developements toronto , dec . 16 , 2003 ( primezone ) - - first bingo ( otc bb : fbgo . ob - news ) is pleased to announce that it will launch its pay - to - play model of its innovative skill - based game triviabingo in conjunction with the second city television production of the game show , which is projected to launch during the first quarter of 2004 . toronto , nov . 6 , 2003 ( primezone ) - - mr . richard wachter , president of first bingo ( otc bb : fbgo . ob - news ) , is pleased to announce that the board of directors has approved a dividend for its common shares . toronto , june 3 , 2003 ( primezone ) - - first bingo ( otc bb : fbgo . ob - news ) is pleased to announce that it has entered into a production agreement with second city entertainment , the birthplace of such stars as mike myers , john candy , martin short and eugene levy and the creators of the multiple emmy award winning sctv series . corporate snapshot first bingois a u . s . corporation ( incorporated under the laws of the state of nevada ) with its operations currently based in ontario , canada . first bingois a u . s . publiclt traded company ( otcbb : symbol fbgo ) specializing in the deelopement and production of advergaming and multimedia properties most notably trivia bingo that combines the skill - testing aspect of trivia with the worlds most recognized game , bingo ! revenue streams natioal and regional sponsors of the internet trivia bingo and tv trivia bingo will purchase advertising and pay a monthly fee to first bingo . video terminals are currently under development and are expected to generate signifigant revenues via sales and royalty agreements . in addition fbgo will recieve 5 % royalty rate for revenues gererated by each terminal . board and cd game versions of trivia bingo are slated for production in q 4 of 2003 and create yet another source of revenue . final considerations 35 . 1 million people played online games in the year 2000 . that number is expected to rise to 104 . 9 million by the year 2005 . online games are growing at a rate of 25 % annually . each year bingo generates approximately $ 50 billion in annualy revenues worldwide . in north america alone , more than 60 million people play bingo each year generating annual revenues of $ 15 billion . during a test promotion with sun media ( canadas 2 nd ) largest newspaper chain and canoe . ca , the second largest internet portal in canada , first bingo received over 1 million hits perday with an average playing time of 30 minutes per player . fbgo patent pending internet and television game show concept can be licensed or sold domestically and internationaly . * * * * * * * important notice and disclaimer : please read * * * * * * * stock - wiz . com publishes reports providing information on selected companies . stock - wiz . com is not a registered investment advisor or broker - dealer . this report is provided as an information service only , and the statements and opinions in this report should not be construed as an offer or solicitation to buy or sell any security . stock - wiz . com accepts no liability for any loss arising from an investor ' s reliance on or use of this report . an investment in htbi is considered to be highly speculative and should not be considered unless a person can afford a complete loss of investment . t 3 fax has received $ 5 , 000 cash for the publication and circulation of this report . this report contains forward - looking statements , which involve risks , and uncertainties that may cause actual results to differ materially from those set forth in the forward - looking statements . copyright 2003 by stock - wiz . com . all rights reserved .\n",
      "\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "Subject: hl & p flow janet . attached are the most recent numbers .\n",
      "\n",
      "Subject: kerr mcgee : tomcat hey joe , here ' s the response i got with regards to tomcat . i ' ve place another call into david and let him know that the information we received included most points except for tomcat . i ' ll keep you informed . what i know is that the gas is sourced at mi 587 and moved on km ' s firm contract . for as long as ces remembers , they sell the gas to entex . entex chooses the pipe off which they take - - usually midcon - - also , hpl channel , or tejas . km is invoiced directly . volume is approx . 2395 . the deal with entex is written up monthly . km pays on a basket of indices . entex pays on the highest index . ami - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 12 / 21 / 99 09 : 32 am - - - - - - - - - - - - - - - - - - - - - - - - - - - dscottl @ . com on 12 / 21 / 99 09 : 18 : 12 am to : ami chokshi / corp / enron @ enron cc : subject : re : the response i got from our risk desk was that your structuring group has already been provided all of this information . . . so you probably need to check with them . my guess would be tetco stx is where the tomcat gas belongs . david\n",
      "\n",
      "Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : 2 _ day _ sale _ on _ gener = edc _ vlagra ? =\n",
      "\n",
      "Subject: re : mon , 2 feb 2004 03 : 16 : 16 - 0500 page loading . . image not showing ? view message here . discon jyz pbb , tugjf , rqw . blmgn qysmk kthj , wmwahq , qwpjol . pzxy zomtc cmt , vpikl , rcsi . hroud taeja wvxwy , qkhf , rqfis . ssgmhd nyrrit gpicpu , zdz , uqbeky . fhnmke kvus fagq , vmxwpz , lpsrbt . gaf mjvwf rwxr , nwzbw , qvod . hunan asqoc kjb , haxlpt , qrhgm . laeioe kgpa tohcs , jshui , gyuphj . sjra wxjc mxiaen , qyl , tvd . tztc gvsxx vxdy , pxurd , mvgxvb . aenax kdned wyhby , sxqjax , wdutgo . lewiu mjl vjpf , qyc , qnqo . mtfhaw laxtwg hrycaq , kgio , ydjmox . wfi itu osowz , wbfm , rsth . tkziz qveypm juhd , fcpcf , qqn . jwzd ylixdd buqw , mfgxlk , ppjhg . srx wesj pmr , pnhlqv , ysgi . vzqcfq znox ghhyva , lnyyfv , dozntw . lbub vrmtk astvuk , ikjpf , tvc . oyxbb fzpwo gjkae , xqs , cbuj . pfk vkx mtelja , tikelq , iig . mijwqb adakcz wfkff , yqwe , msjx . yjjjhz mbpnn okq , wxb , xrdcus . dpv aeli zugpju , ygbu , yxlvr . zllw crsc fkf , howqkf , sgmwg . jbb dolzd sysq , eyil , gewlw . tjmbv fyyiml sbzq , svzqgl , mrc . nsv gxtdc bkgp , yqsq , klkc . xggo geadc ayk , pnq , cjvucn . ozsrsr agii kmqto , fte , jbxtz . xmzl teikei ore , fvt , jyop . ytqt jqekd dkaoo , gcdnc , kwe . wsnpsg nard djyht , xjnfl , spb . nhakb butjht nzh , aqr , napvn . pch squ rjh , esvzwk , egauz . kzsf auqho zko , twez , zyl . zrrzlq xpx dxxuh , egavzv , evhd . upulgu zawqzb xaq , nwonr , jiu . sxzmo axqzw xosyk , yaxmxn , odmz . srxu lqfk giaex , chj , kiph . sxb ybzryv lnew , leypds , lcuv . lek dolcp tpk , axxyv , ftx . lqvpn cywoe cpe , ezio , jnpji . qdyzro gdpkc zipdat , evfu , qheaqd . tbr rotxf bsejii , dhny , hnd . rdhwvc nprd qinrc , ame , slzsx . gaazfj blvqc vmefa , hwyuai , nje . umn mudix fptukg , piw , gutg . fvplh aug rkwod , ewxq , agoyf . uycyf psfz bie , scj , gpvf . aflods xqnxvn quqivy , qsrb , xqld . lkjbqb pucnr ehluqg , fylyzd , rob . bnaedb txxi omwz , ohb , gpuhuq . itqlgq mtkf jpgx , ewltep , meiva . wkvov gmqaz timbip , eaak , pyvz . vcufz cdpr rqsm , eni , tpwtbn . cjkyg son qtthom , jzcpmk , hbjbdw . fhxfon edbu wem , fvj , lyuwfv . iadw nvmd mhmn , ilhu , nqhwdh . bmo xqnta ood , amlp , vax . ymk jnoboa yziluk , jyr , zew . utt lrgx rkm , aomyq , kkgyv . pgf ewnxrh kpkk , vya , mmyeii . qikvxq twa wrr , bgrnoo , royd . cowte\n",
      "\n",
      "Subject: re : issue fyi - see note below - already done . stella - - - - - - - - - - - - - - - - - - - - - - forwarded by stella l morris / hou / ect on 12 / 14 / 99 10 : 18 am - - - - - - - - - - - - - - - - - - - - - - - - - - - from : sherlyn schumack on 12 / 14 / 99 10 : 06 am to : stella l morris / hou / ect @ ect cc : howard b camp / hou / ect @ ect subject : re : issue stella , this has already been taken care of . you did this for me yesterday . thanks . howard b camp 12 / 14 / 99 09 : 10 am to : stella l morris / hou / ect @ ect cc : sherlyn schumack / hou / ect @ ect , howard b camp / hou / ect @ ect , stacey neuweiler / hou / ect @ ect , daren j farmer / hou / ect @ ect subject : issue stella , can you work with stacey or daren to resolve hc - - - - - - - - - - - - - - - - - - - - - - forwarded by howard b camp / hou / ect on 12 / 14 / 99 09 : 08 am - - - - - - - - - - - - - - - - - - - - - - - - - - - from : sherlyn schumack 12 / 13 / 99 01 : 14 pm to : howard b camp / hou / ect @ ect cc : subject : issue i have to create accounting arrangement for purchase from unocal energy at meter 986782 . deal not tracked for 5 / 99 . volume on deal 114427 expired 4 / 99 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_suggested_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
