{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing and Text Mining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Email Spam Classification Demo\n",
    "\n",
    "\n",
    "\n",
    "## By: Asare & Ashikur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Dependencies\n",
    "# 1. numpy\n",
    "# 2. scipy\n",
    "# 2. scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate,cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer,scale\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import grammar_check\n",
    "\n",
    "\n",
    "\n",
    "class EmailClassifier():\n",
    "    \"\"\"\n",
    "     Classifies emails into spam or ham based on several features \n",
    "     The Classifier uses the Support Vector Machine, Random Forest, Naive Bayesian, K Nearest Neighbour\n",
    "     The classifier using cross validation in training and evaluation, and also implements a majority votinig rule\n",
    "     to classify emails into spam or ham\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # run this method onces, and then load the saved data and use subsequently\n",
    "    # method saves dict_enron.npy, all_email_corpus\n",
    "   \n",
    "    def make_Dictionary(root_dir):\n",
    "        all_email_corpus = {'text': [], 'class': []}\n",
    "\n",
    "        emails_dirs = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        all_words = []\n",
    "        for emails_dir in emails_dirs:\n",
    "            dirs = [os.path.join(emails_dir, f) for f in os.listdir(emails_dir)]\n",
    "            for d in dirs:\n",
    "                emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "                for mail in emails:\n",
    "                    with open(mail) as m:\n",
    "                        email_words = []\n",
    "                        for line in m:\n",
    "                            words = nltk.word_tokenize(line)  # line.split()\n",
    "                            all_words += words\n",
    "                            email_words += words\n",
    "                        emailClass = 'ham'\n",
    "                        print mail.split(\".\")[-2]\n",
    "                        if mail.split(\".\")[-2] == 'spam':\n",
    "                            emailClass = 'spam'\n",
    "                        all_email_corpus['text'].append(' '.join(email_words))\n",
    "                        all_email_corpus['class'].append(\n",
    "                            emailClass)  # 1 is spam , 0 is ham\n",
    "\n",
    "        dictionary = Counter(all_words)\n",
    "        list_to_remove = dictionary.keys()\n",
    "\n",
    "        for item in list_to_remove:\n",
    "            if item.isalpha() == False:\n",
    "                del dictionary[item]\n",
    "            elif len(item) == 1:\n",
    "                del dictionary[item]\n",
    "        dictionary = dictionary.most_common(3000)\n",
    "        vocabulary = sorted([key for (key, value) in dictionary])\n",
    "        np.save('vocabulary.npy', vocabulary)\n",
    "        np.save('all_email_corpus.npy', all_email_corpus)\n",
    "\n",
    "        return vocabulary, all_email_corpus\n",
    "\n",
    "   \n",
    "    def classify_emails(self,SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables):\n",
    "\n",
    "            classifier_Labels=[\"SVM\",\"Naive Bayesian\",\"Random Forest\",\"K Nearest Neighbour(K=5)\",\"Majority Vote\"]\n",
    "            majorityVoteClassifier=VotingClassifier(estimators=[(\"SVM\",SVM_pipeline),(\"Naive Bayesian\",NB_pipeline),(\"Random Forest\",RF_pipeline),(\"K-Nearest Neigbour\",KNN_pipeline)],voting='hard')\n",
    "            for clf, label in zip([SVM_pipeline, NB_pipeline, RF_pipeline,KNN_pipeline, majorityVoteClassifier], classifier_Labels):\n",
    "                scoring = ['accuracy','precision', 'recall', 'f1']\n",
    "                crossValidationResults = cross_validate(clf, documents, binarisedLables, cv=5, scoring=scoring,return_train_score=False)\n",
    "                accuracy=crossValidationResults['test_accuracy']\n",
    "                precision=crossValidationResults['test_precision']\n",
    "                recall=crossValidationResults['test_recall']\n",
    "                f1=crossValidationResults['test_f1']\n",
    "                metrics = {}\n",
    "                metrics[\"Classifier\"] = label\n",
    "                metrics[\"Recall\"] = \"%0.2f (+/- %0.2f)\" % (recall.mean(),recall.std())\n",
    "                metrics[\"Precision\"] = \"%0.2f (+/- %0.2f)\" % (precision.mean(),precision.std())\n",
    "                metrics[\"F1 Score\"] = \"%0.2f (+/- %0.2f)\" % (f1.mean(),f1.std())\n",
    "                metrics[\"Accuracy\"] = \"%0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std())\n",
    "\n",
    "            \n",
    "\n",
    "                y_pred = cross_val_predict(clf, documents, binarisedLables)\n",
    "                allSpam=[index for index,value in enumerate(y_pred) if value==1]\n",
    "                allHam=[index for index,value in enumerate(y_pred) if value==0]\n",
    "                spamCount= len(allSpam)\n",
    "                HamCount= len(allHam)\n",
    "                metrics[\"Spam Count\"] = spamCount\n",
    "                metrics[\"Ham Count\"] = HamCount\n",
    "                print metrics\n",
    "\n",
    "                if label==\"Majority Vote\":\n",
    "                    sampledSpam=random.sample(allSpam,5)\n",
    "                    print \"\\n\\n5 randomly sample spam examples\\n\"\n",
    "                    for entry in sampledSpam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                        print str(entry)+\"\\n\"\n",
    "                        emailType=binarisedLables[entry]\n",
    "                        if emailType==1:\n",
    "                            print (\"It is actually a spam\")\n",
    "                        if emailType==0:\n",
    "                            print (\"It is actually a ham\")\n",
    "                            \n",
    "                    sampledHam=random.sample(allHam,5)\n",
    "                    print \"\\n\\n5 randomly sample ham examples\\n\"\n",
    "                    print binarisedLables\n",
    "                    for entry in sampledHam:\n",
    "                        print documents[entry]+\"\\n\"\n",
    "                        print str(entry)+\"\\n\"\n",
    "                        emailType=binarisedLables[entry]\n",
    "                        if emailType==1:\n",
    "                            print (\"It is actually a spam\")\n",
    "                        if emailType==0:\n",
    "                            print (\"It is actually a ham\")\n",
    "                           \n",
    "                        \n",
    "                        \n",
    "                scoring=[]\n",
    "                accuracy=[]\n",
    "                recall=[]\n",
    "                precision=[]\n",
    "                f1=[]\n",
    "                metrics={}\n",
    "                crossValidationResults={}\n",
    "                allHam=[]\n",
    "                allSpam=[]\n",
    "                \n",
    "    \n",
    "    def evaluate_prediction(self,labels_test, predictions):\n",
    "        evaluationTable = []\n",
    "        for key, value in predictions.iteritems():\n",
    "            confusion_matrix\n",
    "            evaluation = {}\n",
    "            evaluation[\"Classifier\"] = key\n",
    "\n",
    "            evaluation[\"Recall\"] = recall_score(labels_test, value)\n",
    "            evaluation[\"Precision\"] = precision_score(labels_test, value)\n",
    "            evaluation[\"F1 Score\"] = f1_score(labels_test, value)\n",
    "            evaluation[\"Average Precision score\"] = average_precision_score(\n",
    "                labels_test, value)\n",
    "            evaluation[\"tn\"],evaluation[\"fp\"],evaluation[\"fn\"],evaluation[\"tp\"] = confusion_matrix(labels_test, value).ravel()\n",
    "            evaluationTable.append(evaluation)\n",
    "        return evaluationTable\n",
    "\n",
    "    def using_TF(self):\n",
    "        \"\"\"Uses term frequency feature for classification\"\"\"\n",
    "        print \"With TF\"\n",
    "        word2vectTransformer=CountVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tf',word2vectTransformer),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    def using_TFIDF(self):\n",
    "        \"\"\"Uses term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "        print \"With TF.IDF\"\n",
    "\n",
    "        java_path = \"C:/Program Files/Java/jdk-9.0.1/bin/java.exe\"\n",
    "        os.environ['JAVAHOME'] = java_path\n",
    "        nltk.internals.config_java(java_path)\n",
    "\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_F1_F2_F3_F4_F5(self):\n",
    "        \"\"\"Uses the following features for classification,\n",
    "            - Counts of Urls\n",
    "            - Count of Language Mistakes\n",
    "            - Count of words\n",
    "            - Count of named entities\n",
    "        \"\"\"\n",
    "\n",
    "        print \"With feature set F1.F2.F3.F4\"\n",
    "\n",
    "        #Note LanguateMistakesVectorizer requires a running grammar-check server. check readme.md\n",
    "        featureSet=FeatureUnion([\n",
    "            ('Count of URLs',URLCountVectorizer()),\n",
    "            ('Count of Language Mistakes',LanguageMistakesVectorizer()),\n",
    "            ('Count of words',WordCountVectorizer()),\n",
    "            ('Count of Named Entities',NameEntityCountVectorizer())\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('feature set',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Components of term frequency x Inverse document frequency feature for classification\"\"\"\n",
    "\n",
    "        print \"With PCA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('PCA',PCA(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses linear discriminant analyses of term frequency x Inverse document \n",
    "        frequency feature for classification\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Using LDA (TF.IDF)\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10)),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('LDA',LinearDiscriminantAnalysis(n_components=10)),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_PCA_and_LDA_of_TFIDF(self):\n",
    "        \"\"\"Uses 10 Principal Componets and Linear discriminant analyses of\n",
    "         term frequency x Inverse document frequency feature for classification\n",
    "         \"\"\"\n",
    "        documents2TfidfVector =TfidfVectorizer(vocabulary=vocabularyList,decode_error='ignore')\n",
    "        print \"Using PCA (TF.IDF), LDA (TF.IDF)\"\n",
    "        \n",
    "        featureSet=FeatureUnion([\n",
    "            ('PCA (TF.IDF)',PCA(n_components=10)),\n",
    "            ('LDA',LinearDiscriminantAnalysis( n_components=10))\n",
    "        ])\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Non Neg Scalling',PCAScaleTranformer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('tfIdf',documents2TfidfVector),\n",
    "            ('to_dense', DenseTransformer()), \n",
    "            ('featureset',featureSet),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "    \n",
    "    \n",
    "    def using_suggested_features(self):\n",
    "        \"\"\"Uses targetted word list occurrence counts feature for classification\"\"\"\n",
    "\n",
    "        print \"Using target workds and symbols €,£,$,%,! viagra, penis, billion, billionaire, lottery, prize, charity , USA, Nigeria\"\n",
    "\n",
    "        SVM_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',LinearSVC()) \n",
    "        ])\n",
    "        NB_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('SVM',MultinomialNB()) \n",
    "        ])\n",
    "        RF_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',RandomForestClassifier()) \n",
    "        ])\n",
    "        KNN_pipeline=Pipeline([\n",
    "            ('targeted_words',TargetedWordCountVectorizer()),\n",
    "            ('Random Forest',KNeighborsClassifier()) \n",
    "        ])\n",
    "\n",
    "        self.classify_emails(SVM_pipeline,NB_pipeline,RF_pipeline,KNN_pipeline,documents,binarisedLables)\n",
    "\n",
    "\n",
    "class WordCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts word counts in the document\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_words_doc(self, doc):\n",
    "        \"\"\"Returns the count of words in a document\"\"\"\n",
    "        all_words = nltk.word_tokenize(doc)\n",
    "        all_words_iter = all_words\n",
    "        for item in all_words_iter:\n",
    "            if item.strip().isalpha() == False:\n",
    "                all_words.remove(item)\n",
    "\n",
    "        return len(all_words)\n",
    "\n",
    "    def get_all_word_counts(self, docs):\n",
    "         \"\"\"Encodes document to number of words\"\"\"\n",
    "         return [self.count_words_doc(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList = self.get_all_word_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class NameEntityCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes a document and extracts count of named entities.\n",
    "        This class uses the NLTK parts of speach tagger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def count_named_entities(self, doc):\n",
    "        \"\"\"Returns a count of named entities in te a document\"\"\"\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        \n",
    "        #serFile=os.path.join(dir_path,'english.all.3class.distsim.crf.ser')\n",
    "        #jarFile=os.path.join(dir_path,'stanford-ner.jar')\n",
    "        #st=StanfordNERTagger(serFile,jarFile,encoding='utf-8')\n",
    "        #tokens_NER=tokens\n",
    "        #for tk in tokens:\n",
    "        #    if len(tk)==1:\n",
    "        #        tokens_NER.remove(tk)\n",
    "        #word_tag=st.tag(tokens_NER)\n",
    "    \n",
    "        named_entities_tag =[]\n",
    "        #for tag in word_tag:\n",
    "        #    if tag[1]!='O':\n",
    "        #        named_entities_tag.append(tag)\n",
    "    \n",
    "        \n",
    "\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        \n",
    "        ##NN\tNoun, singular or mass\n",
    "        ##NNS\tNoun, plural\n",
    "        ##NNP\tProper noun, singular\n",
    "        ##NNPS\tProper noun, plural\n",
    "        #namedEntityTags_set1 = [ \"NN\",\"NNS\" ]#\"NNPS\"\"NN\",\"NNS\"\"NNP\",\"NNPS\"\n",
    "        namedEntityTags_set2 = [ \"NNP\" ]#\"NNPS\"\"NNPS\"\n",
    "        named_entities = []\n",
    "        named_entities_tag =[]\n",
    "       \n",
    "        for word, tag in pos:\n",
    "            if tag in namedEntityTags_set2:\n",
    "                if word.isalpha() and len(word)>1:\n",
    "                   named_entities_tag.append(tag)\n",
    "                   named_entities.append(word)\n",
    "       # print named_entities\n",
    "\n",
    "    \n",
    "        return len(named_entities_tag)\n",
    "\n",
    "    def get_all_named_entities(self, docs):\n",
    "        \"\"\"Encodes document to number of named entities\"\"\"\n",
    "        return [self.count_named_entities(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_named_entities(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class URLCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of URLs,links in document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_url_links(self, s):\n",
    "        \"\"\"Returns number of emails found in text\"\"\"\n",
    "        express1=\"((?:(http s?|s?ftp):\\/\\/)?(?: www \\.)?((?:(?:[A-Z0-9][A-Z0-9-]{0,61}[A-Z0-9]\\.)+)([A-Z]{2,6})|(?:\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}))(?::(\\d{1,5}))?(?:(\\/\\S+)*))\"\n",
    "        express2=\"http [s]?:// (?: www \\.)? (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        express3=\"http|www|goto\"\n",
    "\n",
    "        re1='(http|https|goto)'\t# Word 1\n",
    "        re2='(\\\\s+)'\t# White Space 1\n",
    "        re3='(.)'\t# Any Single Character 1\n",
    "        re4='(\\\\s+)'\t# White Space 2\n",
    "        re5='(.)'\t# Any Single Character 2\n",
    "        re6='(\\\\s+)'\t# White Space 3\n",
    "        re7='(\\\\/)'\t# Any Single Character 3\n",
    "        re8='.*?'\t# Non-greedy match on filler\n",
    "        re9='((www)*)'\t# Word 2\n",
    "        re10='(\\\\s+)'\t# White Space 4\n",
    "        regex = re.compile(re1+re2+re3+re4+re5+re6+re7+re8+re9+re10,re.IGNORECASE|re.DOTALL)\n",
    "        \n",
    "        emails=re.findall(regex, s)\n",
    "\n",
    "        return len(emails)\n",
    "\n",
    "    def get_all_url_counts(self, docs):\n",
    "        \"\"\"Encodes document to number of URL, links\"\"\"\n",
    "        \n",
    "        return [self.count_url_links(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_url_counts(docs)\n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class TargetedWordCountVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts the count of targeted words in the each document\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_targeted_words(self,doc):\n",
    "        target_words=[\"£\",\"$\",\"%\",\"!\" \"viagra\", \"penis\", \"billion\", \"billionaire\", \"lottery\", \"prize\", \"charity\" , \"USA\", \"Nigeria\"]\n",
    "        target_hit_count= len([ word for word in target_words if word in doc ])\n",
    "        return target_hit_count\n",
    "        \n",
    "    def get_all_targeted_words_count(self,docs):\n",
    "        return [self.count_targeted_words(d) for d in docs]\n",
    "        \n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_targeted_words_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class LanguageMistakesVectorizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes a list of documents and extracts count of English language Mistakes\"\"\"\n",
    "    _defaultLanguage='en-GB'\n",
    "    def __init__(self,language=None):\n",
    "        if language:\n",
    "           self._defaultLanguage=language\n",
    "        pass\n",
    "    def count_language_mistakes(self, doc):\n",
    "        \"\"\"Returnes the count of English mistakes in the text \"\"\"\n",
    "        tool=grammar_check.LanguageTool(self._defaultLanguage)\n",
    "        encodedText=doc.decode(\"utf-8\",errors='replace')\n",
    "        try:\n",
    "            mistakes=tool.check(encodedText)\n",
    "            \n",
    "        except Exception  as e:\n",
    "            mistakes=[]\n",
    "        \n",
    "        \n",
    "        return len(mistakes)\n",
    "    def get_all_mistake_count(self,docs):\n",
    "         \"\"\"Encodes document to number of Language mistakes\"\"\"\n",
    "         return [self.count_language_mistakes(d) for d in docs]\n",
    "\n",
    "    def transform(self, docs, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        resultList=self.get_all_mistake_count(docs)\n",
    "       \n",
    "        return np.transpose(np.matrix(resultList))\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \"Takes a sparse array and converts it to dense array\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "    \n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "class PCAScaleTranformer(TransformerMixin):\n",
    "    \"Takes PCA Matrix and coverts to entries of absolute values\"\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "       \n",
    "        return np.absolute(X)\n",
    "        \n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read saved corpus and Initialise Email Classifier \n",
    "\n",
    "### Limited dataset 200 emails (100 spam , 100 ham)\n",
    "#### This dataset has been loaded and said to a numpy array using make_Dictionary in EmailClassifier class above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_email_corpus=np.load(\"./old/all_email_corpus.npy\").item()\n",
    "vocabularyList=np.load(\"./old/vocabulary.npy\").tolist()\n",
    "documents=all_email_corpus['text']\n",
    "labels=all_email_corpus['class']\n",
    "binarizer=LabelBinarizer()\n",
    "binarisedLables=binarizer.fit_transform(labels).ravel()\n",
    "\n",
    "\n",
    "emailclassifier= EmailClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using TF.IDF feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TF.IDF\n",
      "{'Recall': '0.99 (+/- 0.02)', 'Precision': '0.97 (+/- 0.04)', 'Spam Count': 107, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'SVM', 'Ham Count': 93, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.91 (+/- 0.06)', 'Precision': '1.00 (+/- 0.00)', 'Spam Count': 91, 'F1 Score': '0.95 (+/- 0.03)', 'Classifier': 'Naive Bayesian', 'Ham Count': 109, 'Accuracy': '0.95 (+/- 0.03)'}\n",
      "{'Recall': '0.92 (+/- 0.07)', 'Precision': '0.91 (+/- 0.06)', 'Spam Count': 104, 'F1 Score': '0.92 (+/- 0.06)', 'Classifier': 'Random Forest', 'Ham Count': 96, 'Accuracy': '0.92 (+/- 0.06)'}\n",
      "{'Recall': '0.97 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 93, 'F1 Score': '0.98 (+/- 0.03)', 'Classifier': 'K Nearest Neighbour(K=5)', 'Ham Count': 107, 'Accuracy': '0.98 (+/- 0.03)'}\n",
      "{'Recall': '0.96 (+/- 0.04)', 'Precision': '0.99 (+/- 0.02)', 'Spam Count': 96, 'F1 Score': '0.97 (+/- 0.03)', 'Classifier': 'Majority Vote', 'Ham Count': 104, 'Accuracy': '0.97 (+/- 0.03)'}\n",
      "\n",
      "\n",
      "5 randomly sample spam examples\n",
      "\n",
      "Subject: ^ . pe , nis s ^ ize mat ; ters ! yhvqbvdboevkcd briababhdpr frdjvdbesk cdpizacqjkufx hfkosxcymgftzd wdyiwpbqipv xxieqncfpa the only solution to penis enlargement fxbekdcaolk gsiaagcrhyp limited offer : add at least 3 inches or get your money back ! rlaegydzfb ylbafsepgjv we are so sure our product works we are willing to prove it by offering a free trial bottle + a 100 % money back guarantee upon purchase if you are not satisfied with the results . - - - > click here to learn more - - - also check out our * brand new * product : penis enlargement patches comes with the 100 % money back warranty as well ! eqiupgbbaxz gogqkkdpbdo igjohodzauuuu yreliodctrin cbywdvdthl nogsvvbnwug no more offers\n",
      "\n",
      "109\n",
      "\n",
      "It is actually a spam\n",
      "Subject: relax your muscle with the world ' s smallest massager introducing the world ' s smallest wireless massager this fda approved product can release pain and muscle soar , tighten skin , give you a younger look . the technology is based on ancient chinese accupuncture techniques . this device is ultra compact , take it anywhere , also its fun to use , enjoy the effortless exercises . for more information : for future mailing options : a job applicant challenged the interviewer to an arm wrestle .\n",
      "\n",
      "152\n",
      "\n",
      "It is actually a spam\n",
      "Subject: grate still no luck enlarging it ? our 2 products will work for you ! 1 . # 1 supplement available ! - works ! for vprx ciilck here and 2 . * new * enhancement oil - get hard in 60 seconds ! amazing ! like no other oil you ' ve seen . for vprx oil ciilck here the 2 products work great together for women only : ciilck here not intreseted schulz mournful acceptor cavilling acrobacy pack foot lukemia serendipity impresario garfield artifice chink join crimp augusta charley betrayal pollen burg lithic couldn ' t candace circuitous dutiful champ bipartisan btu brumidi cheese registry cogitate arkansan arrow jewelry alaska alphanumeric equipping prowess diehard electrocardiogram herculean aorta highfalutin boxcar ernie roadside pinhead dilution goodwin shakespearian illogic befoul eelgrass allotropic elgin meteorology heart fledge nirvana pathogen passe exploit hymn contextual because chatham arrange maryland cancelled rubin praise brent ironstone bremsstrahlung hebe bronx capacitor flout joshua civilian belshazzar groundwork carol cambodia occurred mobility blest reflectance great algol cheap glacis apathy gut convict porto flagler bushwhack epoxy pathogen errant handleable balustrade culture patrimony conveyance homogenate abacus ontario f geography mccall defensible hardworking encapsulate deviate dollar phenylalanine hangmen inheritor abrade elude mccoy norway mcconnell aid dominique panel decertify hasp bail expensive gourd hurricane digging hanna conclave catechism innovate clapboard forward quality jr daydream convergent boca avuncular howsomever brazzaville ell crossroad grainy awl honoree hanukkah bacchus befitting fda mane cat calcite platonist grosbeak baseman grad grout euridyce heinz capsule maseru hurtle parenthetic consolation aspirate o ' s inferred controvertible amtrak antler lampoon golden gilbertson postfix crossword morocco default debarring postorder copenhagen duplicable message numismatist pursue minnie census genii croupier process monoid handshake machinery edt divisible annalen buss lumberman apricot excisable ness advisee proletariat mint huber esophagi lummox gibbous\n",
      "\n",
      "143\n",
      "\n",
      "It is actually a spam\n",
      "Subject: your prescription is ready . . oxwq s f e low cost prescription medications soma , ultram , adipex , vicodin many more prescribed online and shipped overnight to your door ! ! one of our us licensed physicians will write an fda approved prescription for you and ship your order overnight via a us licensed pharmacy direct to your doorstep . . . . fast and secure ! ! click here ! no thanks , please take me off your list ogrg z lqlokeolnq lnu\n",
      "\n",
      "101\n",
      "\n",
      "It is actually a spam\n",
      "Subject: [ adv ] merry christmas - joyeux noel - frohe weihnachten - feliz navidad english franais deutsch keep track of the real market value through 3 . 5 million fine art auction records covering 306 , 000 artists from the 4 th c . to present and trace their works at auctions with artprice the world leader in art market information . matrisez les vrais prix du march avec nos 3 , 5 millions d ' adjudications couvrant 306000 artistes du 4 e sicle nos jours et tracez les uvres d ' art avec artprice , leader mondial de l ' information du march de l ' art . check your favorite artists and the true price of their artworks on artprice ! unlimited access : usd 8 . 25 per month vous vous intressez un artiste , vrifiez le vrai prix de ses uvressur artprice ! accs illimit : 8 , 25 eur par mois join our 900 , 000 customers ! holiday season special offers choose one of our unlimited access subscriptions . . . rejoignez nos 900 000 clients offres sp?ciales de fin d ' ann?e nos abonnements en acc?s illimit? . . . the annual expert us $ 41 . 58 per month ( unlimited access ) save us $ / eur 500 the perfect gift for serious art collectors and professionals at the special price of us $ / euro 499 instead of us $ / eur 999 ! holiday season offer only valid until january 15 , 2004 . other offers starting at us $ 8 . 25 per month annuel 100 % internet 16 , 58 eur par mois ( accs illimit ) cadeau ! avec un cadeau spcial fin d ' anne : un artprice annual 2003 gratuit ( valeur 129 euros ) pour vous ou offrir un collectionneur ! valable pour les ftes de fin d ' anne et seulement jusqu ' au 15 janvier 2004 . autres offres partir de 8 , 25 euros par mois free email alert : auction watch service on your listed artists ! sign up for free artprice tour alerte email gratuite : surveillance de vos artistes favoris inscription express gratuite sincerely thierry ehrmann the artprice founder the world leader in art market information leader mondial de l ' information sur le marche de l ' art welt - leader in kunstmarkt - infos 1987 - 2003 thierryehrmann online payments accepted modes de paiement en ligne to remove your email : paliourg @ iit . demokritos . gr please click below : in case the above link does not work you can go to http : / / list . artaddiction . com / or reply to this message as it is . please allow us 72 h for your e - mail to be removed . thank you for your co - operation . pour d?sinscrire votre email : paliourg @ iit . demokritos . gr cliquez ci - dessous : http : / / list . artaddiction . com / ? m = paliourg @ iit . demokritos . gr si le lien ci - dessus ne fonctionne pas , vous pouvez aller sur : http : / / list . artaddiction . com / ou r?pondez svp ? ce message sans en modifier le contenu . votre d?sinscription sera effective dans les 72 h . merci de votre coop?ration . en conformit avec la loi 78 - 17 du 6 / 1 / 78 ( cnil ) , vous pouvez demander ne plus figurer sur notre fichier de routage . pure search s 52 artprice . com - domaine de la source bp 69 - f - 69270 st romain au mont d ' or - rcs : 411 309 198\n",
      "\n",
      "119\n",
      "\n",
      "It is actually a spam\n",
      "\n",
      "\n",
      "5 randomly sample ham examples\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Subject: unify production back - eol deals highlighted the proton machine is up and unify production is now available . for being so patient , path manager will now highlight your eol deals in yellow . thanks , d . n .\n",
      "\n",
      "61\n",
      "\n",
      "It is actually a ham\n",
      "Subject: from mrs . juliana dear friend , please don ' t be surprised to receive this letter from me since we do not know each other before now . i am mrs . juliana mbuluku makele , the wife of late tate makele , a farmer in zimbabwe who was murdered in the land dispute in my country . i got your contact through network online hence decided to write you . before the death of my husband , he made a deposit with a security firm . the consignment was declared to contain gem stones shipped on diplomatic cover to avoid customs detecting the actual contents . the actual content is the sum of us 25 . 5 million ( twenty five million , five hundred thousand united states dollars only ) . this was done when he noticed the looming danger in zimbabwe on land policies . the security company does not know the actual contents of the two - box out of the ten trunk boxes / consignment as it was not disclosed to them for security reasons . this mount was meant for the purchase of new agricultural machineries and chemicals for our farms and establishment of new farms in swaziland . this land problem came when zimbabwean president mr . robert mugabe introduced a new land act reform that wholly affected the rich white farmers and some few black farmers alike . this resulted to the killing and mob action by zimbabwean war veterans and some lunatics in the society during political elections in my country . in fact a lot of people were killed because of this land reform act for which my husband was one of the victims . it is against this background that , all my family members fled zimbabwe for fear of our lives . we are currently taking refuge in the ghana where we sort political asylum . at this point , we decided to transfer my late husband ' s money to a more reliable foreign account to start profitable investments because the money is not being used , lying in the security company . i must let you know that this transaction is free from risk because i have all the necessary documents . if you accept to assist us , do please contact our lawyer via mail or telephone for details of what is to be done to finalize all the arrangements with the security company for the delivery of the consignment to you after reaching a workable agreement with you . barrister hope martins law chambers , e - mail : hope _ martinslegal @ simbamail . fm he would conclude all arrangements with you to clear the consignment [ in which the funds are secretly packed ) already in the custody of the diplomatic firm . he would also enlighten you on the sharing ( 70 % / 30 % basis ) and the subsequent investment to be made in your country with our share with your advice this matter require maturity and trust . regards , mrs . juliana mbuluku for : ( the family ) . please you can read about problems in zimbabwe from the links below :\n",
      "\n",
      "155\n",
      "\n",
      "It is actually a spam\n",
      "Subject: revised 7 th noms - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 01 / 06 / 2000 11 : 20 am - - - - - - - - - - - - - - - - - - - - - - - - - - - royal _ b _ edmondson @ reliantenergy . com on 01 / 06 / 2000 10 : 58 : 31 am to : ami chokshi / corp / enron @ enron cc : subject : revised 7 th noms ( see attached file : egmnom - jan . xls ) love ya . . . . . . miss ya . . . . . - egmnom - jan . xls\n",
      "\n",
      "99\n",
      "\n",
      "It is actually a ham\n",
      "Subject: january - meter 2186 clear lake city gate i have flow without a nom each day at meter 2186 lst = 1375 2 nd = 28 3 rd = 2532 4 th = 5952 i assume that this is entex meter . is this volume captured at meter 2000 ? i will need a nom to support this flow . please advise . thank you .\n",
      "\n",
      "94\n",
      "\n",
      "It is actually a ham\n",
      "Subject: hpl nomination for december 30 , 1999 ( see attached file : hpll 230 . xls ) - hpll 230 . xls\n",
      "\n",
      "65\n",
      "\n",
      "It is actually a ham\n"
     ]
    }
   ],
   "source": [
    "emailclassifier.using_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using PCA of TF.IDF (10 principal components) as a feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "emailclassifier.using_PCA_of_TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using suggested feature set i.e targetted  word list feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emailclassifier.using_suggested_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
